{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Pre=Processing the Dataset"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### Notebook Description\n","This is a notebook that preprocesses the dataset for the project. It is designed to be run in Google Colab,  but can be run locally. If run locally, the dataset all_hourly_data.h5 should be in a folder /data within the project folder. The output of this notebook is"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["About the all_hourly_data.h5 file: This is the output of the MIMIC-Extract pipeline, as described in the paper \"MIMIC-Extract: A Data Extraction, Preprocessing, and Representation Pipeline for MIMIC-III.\" (arXiv:1907.08322). The code is available at: [MIMIC_Extract](https://github.com/MLforHealth/MIMIC_Extract). It is a large file that contains all the hourly data for all patients in the MIMIC-III database. This data file is the result of the pipeline using the default parameters, and is supplied by the authors of the MIMIC-Extract paper. The file is available for download from Google Cloud (with appropriate credentialing from [Physionet](https://mimic.mit.edu/docs/gettingstarted/), and a link is provided on the MIMIC_Extract GitHub page."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["This data will be processed using the MIMIC-III preprocessing pipeline as described in "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The following three cells are used to mount the Google Drive to the Colab notebook. This is only necessary if running in Colab. Skip to the next cell if running locally."]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":264,"status":"ok","timestamp":1681746450482,"user":{"displayName":"Michael Conlin","userId":"06177671838518648738"},"user_tz":420},"id":"fgAZTCGD5cKS","outputId":"185aa992-6f9b-4d94-9aba-1f8848cc1802"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[0m\u001b[01;34msample_data\u001b[0m/\n"]}],"source":["%ls"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j6Go4Y0C5Dsi"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SVUBYAVi5V_O"},"outputs":[],"source":["%cd /content/drive/MyDrive/dl4h_project/DynST/\n","%ls"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"glZmycGu5qlx"},"outputs":[],"source":["import logging\n","\n","from pathlib import Path\n","import numpy as np\n","import pandas as pd\n","from scipy.special import expit, logit"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E66-5KxE5tfL"},"outputs":[],"source":["log = logging.getLogger(__name__)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wfKMH-_K5xMs"},"outputs":[],"source":["class Mimic3Pipeline():\n","    def __init__(\n","        self, work_dir, length_range=(16,128), min_code_count=100, n_vitals=25, seed=28\n","        ):\n","        self.work_dir = work_dir\n","        self.input = pd.HDFStore(work_dir + \"/data/all_hourly_data.h5\")\n","        Path(f\"{work_dir}/data/preprocessed_{seed}\").mkdir(parents=True, exist_ok=True)\n","        self.outpath = f\"data/preprocessed_{seed}\"\n","        self.min_length = length_range[0]\n","        self.max_length = length_range[1]\n","        self.min_code_counts = min_code_count\n","        self.n_vitals = n_vitals\n","        self.stay_lengths = None\n","        self.arrays = {}\n","        # baseline hazard\n","        self.H0 = 0.001\n","        # rate of hazard decay\n","        self.labda = 0.25\n","        self.seed = seed\n","        np.random.seed(seed)\n","        # static coefficients\n","        self.beta = np.random.uniform(0.7, 1.2, size=4)\n","        # dynamic coefficients\n","        self.gamma = np.random.uniform(0.1, 0.3, size=4)\n","        # treatment effect on hazards\n","        self.alpha = -0.5\n","\n","    def run(self):\n","        log.info(\"Beginning pipeline\")\n","        # build index of patients\n","        interventions = self.input[\"interventions\"].reset_index()\n","        stay_lengths = interventions.groupby(\"subject_id\").size()\n","        self.stay_lengths = stay_lengths[\n","            (stay_lengths >= self.min_length) & (stay_lengths <= self.max_length)\n","            ]\n","        self.stay_lengths.name=\"stay_length\"\n","        self.arrays[\"patient_index\"] = self.stay_lengths.index.to_numpy()\n","        self.process_patients_data()\n","        self.process_codes()\n","        self.process_vitals()\n","        self.arrays[\"hourly_index\"] = self.vitals.index.get_level_values(0)\n","        # generate labels\n","        self.features = self.semisynth_features()\n","        # fixed interventions\n","        self.features[\"treated\"] = 1\n","        self.features[\"control\"] = 0\n","        df_sim = self.simulate_treatment(self.features.copy())\n","        self.extract_treatment(df_sim)\n","        df_sim =  self.simulate_outcomes(df_sim)\n","        self.arrays[\"survival\"] = df_sim[\"corrected_survival\"].to_numpy()\n","        self.arrays[\"hazards\"] = df_sim[\"hazard\"].to_numpy()\n","        self.summary_statistics(df_sim)\n","\n","        log.info(\"Writing data\")\n","        for key, arr in self.arrays.items():\n","            fname = f\"{self.work_dir}/{self.outpath}/{key}.npy\"\n","            np.save(fname, arr)\n","        df_sim.to_csv(f\"{self.work_dir}/{self.outpath}/df_sim{self.seed}.csv\")\n","        df_sim.to_csv(self.work_dir + f\"/data/mimic3_df_{self.seed}.csv\")\n","        log.info(\"Pipeline completed\")\n","\n","        \n","    def extract_treatment(self, simulated):\n","        treatment = simulated.groupby(\"subject_id\")[\"A\"].any().astype(int)\n","        self.arrays[\"treatment\"] = treatment.to_numpy()\n","\n","\n","    def process_patients_data(self):\n","        demog = self.input[\"patients\"]\n","        demog = demog[[\"gender\", \"age\"]]\n","        d = {\"F\":0, \"M\":1}\n","        demog[\"gender\"] = demog[\"gender\"].apply(lambda x: d.get(x)).astype(int)\n","        demog[\"age\"] = demog[\"age\"].clip(upper=90)\n","        demog[\"age\"] = (demog[\"age\"] - demog[\"age\"].mean()) / demog[\"age\"].std()\n","        demog = demog.reset_index().set_index(\"subject_id\")[[\"gender\", \"age\"]]\n","        self.demog = demog.join(self.stay_lengths, how=\"right\")\n","        self.arrays[\"demog\"] = self.demog[[\"gender\", \"age\"]].to_numpy()\n","\n","\n","    def process_codes(self):\n","        log.info(\"Processing codes\")\n","        codes = self.input[\"codes\"].reset_index()[[\"subject_id\", \"icd9_codes\"]].drop_duplicates([\"subject_id\"])\n","        codes = codes.set_index(\"subject_id\").join(self.stay_lengths, how=\"right\")\n","        codes = codes.explode(\"icd9_codes\")\n","        code_counts = codes[\"icd9_codes\"].value_counts()\n","        code_counts = code_counts[code_counts >= self.min_code_counts]\n","        code_counts.name = \"count\"\n","        code_counts = code_counts.to_frame()\n","        codes = codes.merge(code_counts, left_on=\"icd9_codes\", right_index=True, how=\"left\")\n","        codes[\"icd9_codes\"] = codes[\"icd9_codes\"].mask(codes[\"count\"].isna())\n","        codes[\"icd9_codes\"] = codes[\"icd9_codes\"].fillna(\"unk\")\n","        self.codes = codes\n","        self.arrays[\"code_index\"] = codes.index.to_numpy()\n","        self.arrays[\"code_lookup\"], self.arrays[\"codes\"] = np.unique(\n","            codes[\"icd9_codes\"], return_inverse=True\n","            )\n","    \n","\n","    def process_vitals(self):\n","        log.info(\"Processing vitals\")\n","        vitals = self.input[\"vitals_labs_mean\"].droplevel(['hadm_id', 'icustay_id'])\n","        vitals.columns = vitals.columns.get_level_values(0)\n","        vitals_list = vitals.notna().sum(0).sort_values(ascending=False).head(self.n_vitals).index\n","        vitals = vitals[vitals_list]\n","        vitals = vitals.fillna(method=\"ffill\")\n","        vitals = vitals.fillna(method=\"bfill\")\n","        mean = np.mean(vitals, axis=0)\n","        std = np.std(vitals, axis=0)\n","        vitals = (vitals - mean) / std\n","        self.vitals = vitals.join(self.stay_lengths, how=\"right\").drop(columns = \"stay_length\")\n","        self.arrays[\"vitals\"] = self.vitals.to_numpy()\n","\n","    def simulate_outcomes(self, df, treatment_col=\"A\"):\n","        t = df.index.get_level_values(1)\n","        df[\"baseline_hazard\"] = self.H0 * np.exp(- self.labda * t)\n","        # apply treatment\n","        # column can be \"A\", \"control\" (all zero), or \"treat\" (all one)\n","        df[\"hazard\"] = df[\"baseline_hazard\"] * np.exp(self.alpha * df[treatment_col])\n","        X = df[[\"gender\", \"hypertension\", \"coronary_ath\", \"atrial_fib\"]]\n","        df[\"hazard\"] *= np.exp((X * self.beta).sum(1))\n","        # temporal interaction\n","        df[\"critical\"] = (df[[\"hypertension\", \"coronary_ath\", \"atrial_fib\"]].sum(1) > 1).astype(int)\n","        df[\"hazard\"] *= np.exp(np.log(1.02) * t * df[\"critical\"])\n","        # time-varying variables\n","        V = df[[\"hematocrit\", \"hemoglobin\", \"platelets\", \"mean blood pressure\"]]\n","        V = V.where(V < 0, 0)**2\n","        V = V.clip(upper=3)\n","        df[\"hazard\"] *= np.exp((V * self.gamma).sum(1))\n","        # stabilize hazards and convert to survival probs\n","        df[\"hazard\"] = df[\"hazard\"].clip(lower = 1e-8, upper=0.1)\n","        df[\"q\"] = 1 - df[\"hazard\"]\n","        df[\"survival_prob\"] = df.groupby(\"subject_id\")[\"q\"].cumprod()\n","        np.random.seed(self.seed)\n","        # add jittering\n","        eps = np.random.normal(loc=0, scale=0.5, size=df[\"survival_prob\"].shape)\n","        df[\"survival_prob\"] = expit(logit(df[\"survival_prob\"]) + eps)\n","        df[\"survives\"] = np.random.binomial(1, df[\"survival_prob\"])\n","        return self.corrected_survival_labels(df)\n","\n","\n","    def simulate_treatment(self, df):\n","        # generate propensity scores\n","        df_flat = df.groupby(level=0).head(1)\n","        df_flat[\"critical\"] = (\n","                df_flat[[\"hypertension\", \"coronary_ath\", \"atrial_fib\"]].sum(1) > 1\n","            ).astype(int).to_numpy()\n","        df_flat[\"propensity\"] = df_flat[\"critical\"] * 0.8 + (1 - df_flat[\"critical\"]) * 0.2\n","        np.random.seed(self.seed)\n","        # randomly assign treatment\n","        df_flat[\"A\"] = np.random.binomial(1, df_flat[\"propensity\"])\n","        df = df.join(df_flat[\"A\"], how=\"left\")\n","        df[\"A\"].fillna(method=\"ffill\", inplace=True)\n","        return df\n","\n","\n","\n","    def semisynth_features(self):\n","        df = self.demog[[\"gender\", \"stay_length\"]]\n","        df[\"stay_length\"] = (df[\"stay_length\"] - df[\"stay_length\"].mean()) / df[\"stay_length\"].std()\n","        conf_codes = self.codes.copy()\n","        conf_codes[\"hypertension\"] = (conf_codes[\"icd9_codes\"] == \"4019\")\n","        conf_codes[\"coronary_ath\"] = (conf_codes[\"icd9_codes\"] == \"41401\")\n","        conf_codes[\"atrial_fib\"] = (conf_codes[\"icd9_codes\"] == \"42731\")\n","        conf_codes = conf_codes.groupby(conf_codes.index)[[\"hypertension\", \"coronary_ath\", \"atrial_fib\"]].any().astype(int)\n","        conf_vitals = self.vitals[[\"hematocrit\", \"hemoglobin\", \"platelets\", \"mean blood pressure\"]]\n","        return df.join(conf_codes).join(conf_vitals)\n","\n","    @staticmethod\n","    def corrected_survival_labels(df):\n","        # identify timestep at which first failure occurs (if applicable)\n","        first_failure = df.reset_index(level=\"hours_in\")\n","        first_failure = first_failure[first_failure[\"survives\"] == 0].groupby(level=0).first()\n","        first_failure = first_failure.set_index(\"hours_in\", append=True)\n","        first_failure[\"first_failure\"] = True\n","        first_failure = first_failure[\"first_failure\"]\n","        # label censored patients\n","        censored = df.reset_index(level=\"hours_in\")\n","        censored = (censored[\"survives\"] == 1).groupby(level=0).all()\n","        censored.name = \"censored\"\n","        # combine\n","        df_sim = df.join(first_failure, how=\"left\")\n","        df_sim = df_sim.reset_index(level=\"hours_in\").join(censored, how=\"left\").\\\n","            set_index(\"hours_in\", append=True)\n","        # get corrected survival labels: 1 until first failure, then zero\n","        df_sim[\"corrected_survival\"] = df_sim[\"first_failure\"]\n","        df_sim[\"corrected_survival\"] = df_sim.groupby(level=0)[\"corrected_survival\"].bfill()\n","        df_sim[\"corrected_survival\"] = df_sim[\"corrected_survival\"].fillna(False)\n","        df_sim[\"corrected_survival\"] = (df_sim[\"corrected_survival\"] | df_sim[\"censored\"]).astype(int)\n","        df_sim[\"corrected_survival\"] = df_sim[\"corrected_survival\"].mask(df_sim[\"first_failure\"].fillna(False), 0)\n","        return df_sim\n","\n","    def summary_statistics(self, df_sim):\n","        n = df_sim.reset_index()[\"subject_id\"].nunique()\n","        c = df_sim[df_sim[\"first_failure\"] == True].shape[0]\n","        tau = 16\n","        log.info(f\"{n:,} total patients\")\n","        log.info(f\"{n - c:,} censored ({100*(n - c)/n:.2f} %)\")\n","        lifetimes = df_sim.groupby(level=0)[\"corrected_survival\"].sum().to_numpy()\n","        treated_ix = df_sim.groupby(level=0)[\"A\"].any()\n","        log.info(f\"Mean time to censoring or failure: {np.mean(lifetimes):.2f} hours\")\n","        rst = self.rmst(df_sim, tau)\n","        log.info(f\"Mean restricted survival time: {np.mean(rst):.2f} hours, tau = {tau}\")\n","        unadj_ate = rst[treated_ix].mean() - rst[~treated_ix].mean()\n","        log.info(f\"Observed treatment effect: {unadj_ate:.2f} hours\")\n","        # calculate true ATE\n","        df_treated = self.simulate_outcomes(self.features, \"treated\")\n","        rmst_treated = np.mean(self.rmst(df_treated, tau))\n","        df_control = self.simulate_outcomes(self.features, \"control\")\n","        rmst_control = np.mean(self.rmst(df_control, tau))\n","        log.info(f\"True treatment effect: {rmst_treated - rmst_control:.2f} hours\")\n","\n","    @staticmethod\n","    def rmst(df, tau):\n","        restr = df.groupby(level=0)[\"corrected_survival\"].head(tau)\n","        rst = restr.groupby(level=0).sum()\n","        return rst.to_numpy()\n"]},{"cell_type":"markdown","metadata":{"id":"BCZV5_Hq78Qd"},"source":["Below is from dataset.py"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_L6p83RL77ph"},"outputs":[],"source":["import os\n","import numpy as np\n","import torch\n","from torch.utils.data import Dataset\n","from torch.nn.utils.rnn import pad_sequence"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M6Wa4lZV8DvP"},"outputs":[],"source":["class Mimic3Dataset(Dataset):\n","    def __init__(self, work_dir, seed, intervention=None):\n","        fdir = f\"{work_dir}/data/preprocessed_{seed}\"\n","        self.f = {}\n","        for fname in os.listdir(fdir):\n","            if fname.endswith(\".npy\"):\n","                self.f[fname[:-4]] = np.load(\n","                    f\"{fdir}/{fname}\", allow_pickle=True\n","                    )\n","        self.ix = self.f[\"patient_index\"]\n","        self.code_lookup = np.insert(self.f[\"code_lookup\"], 0, \"pad\")\n","        self.codes = self.f[\"codes\"] + 1\n","        self.n_codes = len(self.code_lookup)\n","        self.n_vitals = self.f[\"vitals\"].shape[1]\n","        self.n_demog = self.f[\"demog\"].shape[1]\n","        self.pad_value = - 100\n","        # if supplied, represents treatment (True) or control (False)\n","        self.intervention = intervention\n","\n","\n","\n","    def __len__(self):\n","        return len(self.f[\"treatment\"])\n","\n","    def __getitem__(self, index):\n","        item = {}\n","        j = self.ix[index]\n","        if self.intervention is None:\n","            item[\"treatment\"] = self.f[\"treatment\"][index]\n","        else:\n","            item[\"treatment\"] = int(self.intervention)\n","        item[\"demog\"] = self.f[\"demog\"][index]\n","        item[\"codes\"] = torch.tensor(\n","            self.pad_bincount(self.f[\"codes\"][self.f[\"code_index\"] == j])\n","        )\n","        item[\"vitals\"] = torch.tensor(\n","            self.f[\"vitals\"][self.f[\"hourly_index\"] == j]\n","        ).float()\n","        item[\"survival\"] = torch.tensor(\n","            self.f[\"survival\"][self.f[\"hourly_index\"] == j]\n","        )\n","        return item\n","\n","    def pad_bincount(self, records):\n","        # get counts of each cod\n","        records = np.bincount(records)\n","        # pad each vector to length T, all possible codes\n","        padded = np.zeros(self.n_codes)\n","        padded[: len(records)] = records\n","        return torch.from_numpy(padded).float()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1EmhVdED8HwP"},"outputs":[],"source":["def padded_collate(batch, pad_index, causal=False):\n","    res = {}\n","    treatment = torch.tensor(np.array([d[\"treatment\"] for d in batch]))\n","    demog = torch.tensor(np.array([d[\"demog\"] for d in batch])).float()\n","    if causal:\n","        res[\"treatment\"] = torch.tensor(np.array([d[\"treatment\"] for d in batch]))\n","        res[\"static\"] = torch.tensor(np.array([d[\"demog\"] for d in batch])).float()\n","    else:\n","        res[\"static\"] = torch.cat([demog, treatment.unsqueeze(1)], 1)\n","    res[\"codes\"] = torch.stack([d[\"codes\"] for d in batch])\n","    res[\"vitals\"] = pad_sequence(\n","        [d[\"vitals\"] for d in batch], batch_first=True, padding_value=pad_index\n","    )\n","    res[\"survival\"] = pad_sequence(\n","        [d[\"survival\"] for d in batch], batch_first=True, padding_value=pad_index\n","    )\n","    return res"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyM8WWGjf09nZftw1TrG/1I0","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
