{"cells":[{"cell_type":"markdown","metadata":{"id":"EJUTjg5lNK5c"},"source":["## Preprocessing the Dataset"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"XR-lXJedNK5g"},"source":["#### Notebook Description\n","This is a notebook that preprocesses the dataset for the project. It is designed to be run in Google Colab,  but can be run locally. If run locally, the dataset all_hourly_data.h5 should be in a folder \"/data\" within the project folder. The output of the processing step are the csv file which contains the processed and synthetic data, and a series of numpy arrays for use by the Mimic3Dataset class."]},{"cell_type":"markdown","metadata":{"id":"vPdDbdvcNK5g"},"source":["#### MIMIC-III Dataset\n","The MIMIC-III dataset is a large dataset of de-identified health data. It is available from the Physionet website: https://physionet.org/content/mimiciii/1.4/. It contains health related data from patients who were admitted to the critical care units of Beth Isreal Deaconess Medical Center from 2001 to 2012. It consists of both static and dynamic data. In addition to patient demographics, there is data acquired from the hospital that is not necessarily related to the patient's intensive care unit (ICU) stay, and the data related to the ICU stay such as vital signs, laboratory tests, diagnostic codes, medications, etc. This includes data from bedside monitoring systems, and the \"chart\" (electronic health record) data including provider's notes.\n","Patient Characteristics:\n","There are data associated with 53,423 distinct adult (age 16 and above) hospital admissions of 38,597 distinct adult patients. There are also data for 7870 neonate admissions, which we will eliminate from our data for analysis."]},{"cell_type":"markdown","metadata":{"id":"HfE6iMqzNK5h"},"source":["#### MIMIC-Extract data\n","MIMIC Extract is an open source pipeline to \"extract, preprocess, and represent data from MIMIC-III v1.4.\", as described in the paper \"MIMIC-Extract: A Data Extraction, Preprocessing, and Representation Pipeline for MIMIC-III.\" (arXiv:1907.08322). The code is available at: [MIMIC_Extract](https://github.com/MLforHealth/MIMIC_Extract).\n","The extraction process results in data that includes well-formatted time-series data for clinically-meaningful prediction tasks. The time-varying data are discretized into hourly buckets, standardized, and aggregated into clinically meaningful representations.\n","The cohort of patients includes all adult patients whose ICU stay is their first, and that stay lasts at least 12 hours, and less that 10 days. This represents a generic cohort, not task-specific, and therefore can be adapted for many different clinical prediction tasks.\n","The extraction procedure is extensible and you may configure the cohort selection process. Using the default parameters for cohort selection, the data consists of 34,472 patients.About the all_hourly_data.h5 file:  It is a large file that contains all the hourly data for all patients in the MIMIC-III database. The \"all_hourly_data.h5\" file is the result of the pipeline using the default parameters, and is supplied by the authors of the MIMIC-Extract paper. The file is available for download from Google Cloud (with appropriate credentialing from [Physionet](https://mimic.mit.edu/docs/gettingstarted/)), and a link is provided on the MIMIC_Extract GitHub page.\n","\n","The h5 datafile format is a hierarchical data format that is optimized for storing large amounts of data. It is a binary format, and is not human-readable. The data is stored in a hierarchical filesytem-like format, with each node in the hierarchy being a group or a dataset. A group is a container that can hold datasets and other groups. A dataset is a multidimensional array of data elements. The data can be read directly into a pandas dataframes, which is what we will do in this notebook."]},{"cell_type":"markdown","metadata":{"id":"Vav2wW9CNK5i"},"source":["The following three cells are used to mount the Google Drive to the Colab notebook. This is only necessary if running in Colab. Skip to the next cell if running locally."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":188,"status":"ok","timestamp":1682373908300,"user":{"displayName":"Michael Conlin","userId":"06177671838518648738"},"user_tz":420},"id":"fgAZTCGD5cKS","outputId":"e77d0f51-2b63-44f7-d97f-f0dc1b3d34de"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[0m\u001b[01;34msample_data\u001b[0m/\n"]}],"source":["# Where are we?\n","%ls"]},{"cell_type":"markdown","metadata":{"id":"HlTlJ1xcJj8S"},"source":["### Mount Google Drive\n","I am setting this project up to run in Google Colab, and I am using Google Drive to store the project code and data. The following cell mounts the Google Drive to the Colab notebook."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17047,"status":"ok","timestamp":1682373927845,"user":{"displayName":"Michael Conlin","userId":"06177671838518648738"},"user_tz":420},"id":"j6Go4Y0C5Dsi","outputId":"5cdc854e-cda0-4dc7-ca17-06f298892760"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["# Mount our Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":503,"status":"ok","timestamp":1682373929548,"user":{"displayName":"Michael Conlin","userId":"06177671838518648738"},"user_tz":420},"id":"SVUBYAVi5V_O","outputId":"092477b9-6fe8-4902-d64b-9cf6c5474d9a"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/dl4h_project/DynST\n","causal.ipynb            \u001b[0m\u001b[01;34moutputs\u001b[0m/                          pyproject.toml\n","config.yaml             poetry.lock                       README.md\n","coxph_model.ipynb       project_causal.ipynb              results20230416.txt\n","\u001b[01;34mdata\u001b[0m/                   project_coxph_model.ipynb         run.py\n","dl4hProjectSetup.ipynb  project_model.ipynb               \u001b[01;34msrc\u001b[0m/\n","\u001b[01;34mmultirun\u001b[0m/               project_preprocess_dataset.ipynb\n"]}],"source":["# Change to the project directory - yours may be different\n","%cd /content/drive/MyDrive/dl4h_project/DynST/\n","%ls"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"glZmycGu5qlx"},"outputs":[],"source":["import logging\n","\n","from pathlib import Path\n","import numpy as np\n","import pandas as pd\n","from scipy.special import expit, logit"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E66-5KxE5tfL"},"outputs":[],"source":["log = logging.getLogger(__name__)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"HeHW5jjYbx2x"},"source":["### Mimic3Pipeline Class\n","This class contains functions which prepare the data for analysis. The source of the data is the h5 file that is the output of the MIMIC-Extract pipeline. The data is read into a pandas dataframes, and processed to prepare it for analysis. The processing steps include:\n","- filtering out stays that are less than 16 hours or greater than 128\n","- replace age values > 90 with 90\n","- standardize the continuous variables\n","- create the synthetic hazard function according to the formula in the paper:\n","  $$h(t) = H_o exp(-\\lambda t) * exp(\\theta A) * exp(\\sum_{j=1}^{4}\\beta_j Z_j)* exp(log(1.02)tZ_*) * exp(\\sum_{j=1}^{4}\\gamma_j g(V_j^{(t)}))$$\n","  where:\n","    - $H_o$ is the baseline hazard function\n","    - \\lambda is the decay rate of the hazard function (risk decreases with time)\n","    - A is the binary treatment indicator, and $\\theta$ is the treatment effect (treatment decreases risk)\n","    - $Z_j$ are the four static variables, and $\\beta_j$ are the coefficients for the static variables\n","    - $Z_*$ is the indicator for severe illness, and $tZ_*$ is the interaction term between $Z_*$ and time\n","    - $V_j^{(t)}$ are the four time-varying variables (hematocrit, hemoglobin, platelets, mean blood pressure), and $\\gamma_j$ are the coefficients for the time-varying variables\n","  - calculate the survival probabilites for each patient at each hour\n","  - using the survival probabilities, calculate the survival status for each patient at each hour\n","  - calculate the average treatment effect (ATE)\n","\n","The output of this class is the csv file which contains the processed and synthetic data, and a series of numpy arrays for use by the Mimic3Dataset class."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wfKMH-_K5xMs"},"outputs":[],"source":["class Mimic3Pipeline():\n","    def __init__(\n","        self, work_dir, length_range=(16,128), min_code_count=100, n_vitals=25, seed=28\n","        ):\n","        self.work_dir = work_dir # passed in as parameter \n","        self.input = pd.HDFStore(work_dir + \"/data/all_hourly_data.h5\")\n","        Path(f\"{work_dir}/data/preprocessed_{seed}\").mkdir(parents=True, exist_ok=True)\n","        self.outpath = f\"data/preprocessed_{seed}\"\n","        # minimum and maximum length of stay in hours\n","        self.min_length = length_range[0] # 16\n","        self.max_length = length_range[1] # 128\n","        self.min_code_counts = min_code_count # 100\n","        self.n_vitals = n_vitals # 25\n","        self.stay_lengths = None\n","        # create a dictionary to store arrays of data\n","        self.arrays = {}\n","        # baseline hazard\n","        self.H0 = 0.001\n","        # rate of hazard decay\n","        self.labda = 0.25\n","        self.seed = seed # 28\n","        np.random.seed(seed)\n","        # static coefficients \n","        # from samples of a uniform distribution (every value equally likely to occur)\n","        self.beta = np.random.uniform(0.7, 1.2, size=4)\n","        # dynamic coefficients\n","        self.gamma = np.random.uniform(0.1, 0.3, size=4)\n","        # treatment effect on hazards\n","        self.alpha = -0.5\n","\n","    def run(self):\n","        log.info(\"Beginning pipeline\")\n","        # build index of patients\n","        # create a pandas df of interventions from the input hdf5 file\n","        interventions = self.input[\"interventions\"].reset_index()\n","        # for each patient in the interventions df, \n","        # determine the length of their stay in hours (each column is a different hour)\n","        stay_lengths = interventions.groupby(\"subject_id\").size()\n","        # filter out patients with stays that are too short (<16 hrs) or too long (>128 hrs)\n","        self.stay_lengths = stay_lengths[\n","            (stay_lengths >= self.min_length) & (stay_lengths <= self.max_length)\n","            ]\n","        self.stay_lengths.name=\"stay_length\"\n","        self.arrays[\"patient_index\"] = self.stay_lengths.index.to_numpy()\n","        self.process_patients_data()\n","        self.process_codes()\n","        self.process_vitals()\n","        self.arrays[\"hourly_index\"] = self.vitals.index.get_level_values(0)\n","        # generate labels using the function below\n","        self.features = self.semisynth_features()\n","        # fixed interventions\n","        self.features[\"treated\"] = 1\n","        self.features[\"control\"] = 0\n","        df_sim = self.simulate_treatment(self.features.copy())\n","        self.extract_treatment(df_sim)\n","        df_sim =  self.simulate_outcomes(df_sim)\n","        self.arrays[\"survival\"] = df_sim[\"corrected_survival\"].to_numpy()\n","        self.arrays[\"hazards\"] = df_sim[\"hazard\"].to_numpy()\n","        self.summary_statistics(df_sim)\n","        # save data to csv file\n","        log.info(\"Writing data\")\n","        for key, arr in self.arrays.items():\n","            fname = f\"{self.work_dir}/{self.outpath}/{key}.npy\"\n","            np.save(fname, arr)\n","        df_sim.to_csv(f\"{self.work_dir}/{self.outpath}/df_sim{self.seed}.csv\")\n","        df_sim.to_csv(self.work_dir + f\"/data/mimic3_df_{self.seed}.csv\")\n","        log.info(\"Pipeline completed\")\n","\n","        \n","    def extract_treatment(self, simulated):\n","        # groupby col \"subject_id\" check if values in column \"A\" are True.\n","        # casts the resulting True/False into 1 (True) and 0 (False) using the astype() method.\n","        treatment = simulated.groupby(\"subject_id\")[\"A\"].any().astype(int)\n","        # This line adds the array \"treatment\" to the \"arrays\" dictionary with key \"treatment\".\n","        # to_numpy() method to obtain the Numpy array from the pandas Series object.\n","        self.arrays[\"treatment\"] = treatment.to_numpy()\n","\n","\n","    def process_patients_data(self):\n","        # create a pandas df of patients from the input hdf5 file\n","        demog = self.input[\"patients\"]\n","        # select gender and age columns\n","        demog = demog[[\"gender\", \"age\"]]\n","        d = {\"F\":0, \"M\":1}\n","        # set gender to 0 or 1\n","        demog[\"gender\"] = demog[\"gender\"].apply(lambda x: d.get(x)).astype(int)\n","        # replace values greater than 90 with 90\n","        demog[\"age\"] = demog[\"age\"].clip(upper=90)\n","        # standardize age with mean 0 and std 1 using z-score\n","        demog[\"age\"] = (demog[\"age\"] - demog[\"age\"].mean()) / demog[\"age\"].std()\n","        # set the index of the df (with gender and age columns) to the subject_id\n","        demog = demog.reset_index().set_index(\"subject_id\")[[\"gender\", \"age\"]]\n","        # join the stay_lengths df with the demog df\n","        self.demog = demog.join(self.stay_lengths, how=\"right\")\n","        self.arrays[\"demog\"] = self.demog[[\"gender\", \"age\"]].to_numpy()\n","\n","\n","    def process_codes(self):\n","        log.info(\"Processing codes\")\n","        # create a pandas df of codes (subject_id, icd9_codes) reset the index\n","        codes = self.input[\"codes\"].reset_index()[[\"subject_id\", \"icd9_codes\"]].drop_duplicates([\"subject_id\"])\n","        # join the stay_lengths df with the codes df\n","        codes = codes.set_index(\"subject_id\").join(self.stay_lengths, how=\"right\")\n","        # explode will create a new row for each code if there is a list of codes in a row\n","        codes = codes.explode(\"icd9_codes\")\n","        code_counts = codes[\"icd9_codes\"].value_counts()\n","        # filter out codes that occur less than 100 times\n","        code_counts = code_counts[code_counts >= self.min_code_counts]\n","        code_counts.name = \"count\"\n","        code_counts = code_counts.to_frame()\n","        # merge the code_counts df with the codes df\n","        codes = codes.merge(code_counts, left_on=\"icd9_codes\", right_index=True, how=\"left\")\n","        codes[\"icd9_codes\"] = codes[\"icd9_codes\"].mask(codes[\"count\"].isna())\n","        codes[\"icd9_codes\"] = codes[\"icd9_codes\"].fillna(\"unk\")\n","        self.codes = codes\n","        self.arrays[\"code_index\"] = codes.index.to_numpy()\n","\n","        # np.unique() returns the unique values in the array and the indices of the unique values\n","        # \"code_lookup\" is a list of unique codes, \"codes\" are the indices of the codes in \"code_lookup\"\n","        self.arrays[\"code_lookup\"], self.arrays[\"codes\"] = np.unique(\n","            codes[\"icd9_codes\"], return_inverse=True\n","            )\n","    \n","\n","    def process_vitals(self):\n","        log.info(\"Processing vitals\")\n","        # create a pandas df of vitals, dropping the hadm_id and icustay_id columns\n","        vitals = self.input[\"vitals_labs_mean\"].droplevel(['hadm_id', 'icustay_id'])\n","        # reset the index\n","        vitals.columns = vitals.columns.get_level_values(0)\n","        # create a list of the top n_vitals (25) most common vitals\n","        vitals_list = vitals.notna().sum(0).sort_values(ascending=False).head(self.n_vitals).index\n","        vitals = vitals[vitals_list]\n","        # fill missing values with the previous value\n","        vitals = vitals.fillna(method=\"ffill\")\n","        vitals = vitals.fillna(method=\"bfill\")\n","        mean = np.mean(vitals, axis=0)\n","        std = np.std(vitals, axis=0)\n","        # standardize vitals with mean 0 and std 1 using z-score\n","        vitals = (vitals - mean) / std\n","        self.vitals = vitals.join(self.stay_lengths, how=\"right\").drop(columns = \"stay_length\")\n","        self.arrays[\"vitals\"] = self.vitals.to_numpy()\n","\n","    def simulate_outcomes(self, df, treatment_col=\"A\"):\n","        t = df.index.get_level_values(1)\n","\n","        ###### Synthetic Hazard Function ######\n","\n","        ## baseline hazard ##\n","        # The Lindy effect\n","        # baseline hazard = 0.001 * exp(- 0.25 * t) # using default values, t in hours\n","        df[\"baseline_hazard\"] = self.H0 * np.exp(- self.labda * t)\n","        \n","        ## apply treatment effect ##\n","        # column can be \"A\", \"control\" (all zero), or \"treat\" (all one)\n","        df[\"hazard\"] = df[\"baseline_hazard\"] * np.exp(self.alpha * df[treatment_col])\n","\n","        ## apply static variables ##\n","        # beta is from a uniform(0.7, 1.2) distribution\n","        X = df[[\"gender\", \"hypertension\", \"coronary_ath\", \"atrial_fib\"]]\n","        df[\"hazard\"] *= np.exp((X * self.beta).sum(1))\n","\n","        ## apply temporal interaction ##\n","        # \"criticl\" (Z) is an indicator for severely ill patients, those with 2 or more \n","        # of the following conditions: hypertension, coronary artery disease, atrial fibrillation\n","        # if the patient is severely ill, the hazard increases by 2% per hour,\n","        # thus violating the proportional hazards assumption\n","        df[\"critical\"] = (df[[\"hypertension\", \"coronary_ath\", \"atrial_fib\"]].sum(1) > 1).astype(int)\n","        df[\"hazard\"] *= np.exp(np.log(1.02) * t * df[\"critical\"])\n","\n","        ## time-varying variables ##\n","        V = df[[\"hematocrit\", \"hemoglobin\", \"platelets\", \"mean blood pressure\"]]\n","        # note, pandas.where:\n","        # if V is < 0, keep the original value, otherwise set to 0, then square the result\n","        V = V.where(V < 0, 0)**2\n","        V = V.clip(upper=3)\n","        df[\"hazard\"] *= np.exp((V * self.gamma).sum(1))\n","        \n","        # stabilize hazards and convert to survival probs\n","        # constrain the hazard to be between 1e-8 and 0.1\n","        df[\"hazard\"] = df[\"hazard\"].clip(lower = 1e-8, upper=0.1)\n","        # survival probability at single time step = 1 - hazard\n","        df[\"q\"] = 1 - df[\"hazard\"]\n","        # survival probability at all time steps = cumulative product of survival probabilities\n","        df[\"survival_prob\"] = df.groupby(\"subject_id\")[\"q\"].cumprod()\n","        # add jittering to survival probabilities\n","        np.random.seed(self.seed)\n","        # eps is a random number from a normal distribution with mean 0 and std 0.5\n","        # to add Gaussian noise to the logit of the survival probability\n","        eps = np.random.normal(loc=0, scale=0.5, size=df[\"survival_prob\"].shape)\n","        df[\"survival_prob\"] = expit(logit(df[\"survival_prob\"]) + eps)\n","        # use the patient's survival probability to generate a bernoulli random variable, \n","        # the first step that results in a 0 is the patient's survival time\n","        df[\"survives\"] = np.random.binomial(1, df[\"survival_prob\"])\n","        return self.corrected_survival_labels(df)\n","\n","\n","    def simulate_treatment(self, df):\n","        ## randomize treatment assignment using propensity scores ##\n","        # generate propensity scores\n","        df_flat = df.groupby(level=0).head(1)\n","        df_flat[\"critical\"] = (\n","                df_flat[[\"hypertension\", \"coronary_ath\", \"atrial_fib\"]].sum(1) > 1\n","            ).astype(int).to_numpy()\n","        df_flat[\"propensity\"] = df_flat[\"critical\"] * 0.8 + (1 - df_flat[\"critical\"]) * 0.2\n","        np.random.seed(self.seed)\n","        # randomly assign treatment\n","        df_flat[\"A\"] = np.random.binomial(1, df_flat[\"propensity\"])\n","        df = df.join(df_flat[\"A\"], how=\"left\")\n","        df[\"A\"].fillna(method=\"ffill\", inplace=True)\n","        return df\n","\n","\n","\n","    def semisynth_features(self):\n","        df = self.demog[[\"gender\", \"stay_length\"]]\n","        # standardize stay length\n","        df[\"stay_length\"] = (df[\"stay_length\"] - df[\"stay_length\"].mean()) / df[\"stay_length\"].std()\n","        conf_codes = self.codes.copy()\n","        # indicators for hypertension, coronary artery disease, and atrial fibrillation\n","        conf_codes[\"hypertension\"] = (conf_codes[\"icd9_codes\"] == \"4019\")\n","        conf_codes[\"coronary_ath\"] = (conf_codes[\"icd9_codes\"] == \"41401\")\n","        conf_codes[\"atrial_fib\"] = (conf_codes[\"icd9_codes\"] == \"42731\")\n","        conf_codes = conf_codes.groupby(conf_codes.index)[[\"hypertension\", \"coronary_ath\", \"atrial_fib\"]].any().astype(int)\n","        conf_vitals = self.vitals[[\"hematocrit\", \"hemoglobin\", \"platelets\", \"mean blood pressure\"]]\n","        return df.join(conf_codes).join(conf_vitals)\n","\n","    @staticmethod\n","    def corrected_survival_labels(df):\n","        # identify timestep at which first failure occurs (if applicable)\n","        first_failure = df.reset_index(level=\"hours_in\")\n","        first_failure = first_failure[first_failure[\"survives\"] == 0].groupby(level=0).first()\n","        first_failure = first_failure.set_index(\"hours_in\", append=True)\n","        first_failure[\"first_failure\"] = True\n","        first_failure = first_failure[\"first_failure\"]\n","        # label censored patients\n","        censored = df.reset_index(level=\"hours_in\")\n","        censored = (censored[\"survives\"] == 1).groupby(level=0).all()\n","        censored.name = \"censored\"\n","        # combine\n","        df_sim = df.join(first_failure, how=\"left\")\n","        df_sim = df_sim.reset_index(level=\"hours_in\").join(censored, how=\"left\").\\\n","            set_index(\"hours_in\", append=True)\n","        # get corrected survival labels: 1 until first failure, then zero\n","        df_sim[\"corrected_survival\"] = df_sim[\"first_failure\"]\n","        df_sim[\"corrected_survival\"] = df_sim.groupby(level=0)[\"corrected_survival\"].bfill()\n","        df_sim[\"corrected_survival\"] = df_sim[\"corrected_survival\"].fillna(False)\n","        df_sim[\"corrected_survival\"] = (df_sim[\"corrected_survival\"] | df_sim[\"censored\"]).astype(int)\n","        df_sim[\"corrected_survival\"] = df_sim[\"corrected_survival\"].mask(df_sim[\"first_failure\"].fillna(False), 0)\n","        return df_sim\n","\n","    def summary_statistics(self, df_sim):\n","        n = df_sim.reset_index()[\"subject_id\"].nunique()\n","        c = df_sim[df_sim[\"first_failure\"] == True].shape[0]\n","        tau = 16\n","        log.info(f\"{n:,} total patients\")\n","        log.info(f\"{n - c:,} censored ({100*(n - c)/n:.2f} %)\")\n","        lifetimes = df_sim.groupby(level=0)[\"corrected_survival\"].sum().to_numpy()\n","        treated_ix = df_sim.groupby(level=0)[\"A\"].any()\n","        log.info(f\"Mean time to censoring or failure: {np.mean(lifetimes):.2f} hours\")\n","        rst = self.rmst(df_sim, tau)\n","        log.info(f\"Mean restricted survival time: {np.mean(rst):.2f} hours, tau = {tau}\")\n","        unadj_ate = rst[treated_ix].mean() - rst[~treated_ix].mean()\n","        log.info(f\"Observed treatment effect: {unadj_ate:.2f} hours\")\n","        # calculate true ATE\n","        df_treated = self.simulate_outcomes(self.features, \"treated\")\n","        rmst_treated = np.mean(self.rmst(df_treated, tau))\n","        df_control = self.simulate_outcomes(self.features, \"control\")\n","        rmst_control = np.mean(self.rmst(df_control, tau))\n","        log.info(f\"True treatment effect: {rmst_treated - rmst_control:.2f} hours\")\n","\n","    @staticmethod\n","    def rmst(df, tau):\n","        restr = df.groupby(level=0)[\"corrected_survival\"].head(tau)\n","        rst = restr.groupby(level=0).sum()\n","        return rst.to_numpy()\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Run the preprocessing pipeline"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Import the os module\n","import os\n","\n","# Get the current working directory\n","cwd = os.getcwd()\n","\n","# Print the current working directory\n","print(\"Current working directory: {0}\".format(cwd))\n","# preprocess_seed\n","preprocess_seed = 30\n","pipeline = Mimic3Pipeline(work_dir=cwd, seed=preprocess_seed)\n","pipeline.run()"]}],"metadata":{"colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
