{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"HhXLWyabgZZR"},"source":[" #### Mount Google Drive and change directory to the project folder"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1092,"status":"ok","timestamp":1681671400544,"user":{"displayName":"Michael Conlin","userId":"06177671838518648738"},"user_tz":420},"id":"zdluUkf7gjiw","outputId":"95abac03-747f-44a1-ca3a-e5648eff990d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/dl4h_project/DynST/\n","%ls"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Mimic3Dataset Class\n","This will be called later to create the dataset for training and testing the model. It will read the csv file created by the Mimic3Pipeline class, and create the dataset for training and testing the model.\n","Also provided are the functions to pad and collate the data for use by the model."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","import numpy as np\n","import torch\n","from torch.utils.data import Dataset\n","from torch.nn.utils.rnn import pad_sequence"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class Mimic3Dataset(Dataset):\n","    def __init__(self, work_dir, seed, intervention=None):\n","        fdir = f\"{work_dir}/data/preprocessed_{seed}\"\n","        self.f = {}\n","        for fname in os.listdir(fdir):\n","            if fname.endswith(\".npy\"):\n","                self.f[fname[:-4]] = np.load(\n","                    f\"{fdir}/{fname}\", allow_pickle=True\n","                    )\n","        self.ix = self.f[\"patient_index\"]\n","        self.code_lookup = np.insert(self.f[\"code_lookup\"], 0, \"pad\")\n","        self.codes = self.f[\"codes\"] + 1\n","        self.n_codes = len(self.code_lookup)\n","        self.n_vitals = self.f[\"vitals\"].shape[1]\n","        self.n_demog = self.f[\"demog\"].shape[1]\n","        self.pad_value = - 100\n","        # if supplied, represents treatment (True) or control (False)\n","        self.intervention = intervention\n","\n","\n","\n","    def __len__(self):\n","        return len(self.f[\"treatment\"])\n","\n","    def __getitem__(self, index):\n","        item = {}\n","        j = self.ix[index]\n","        if self.intervention is None:\n","            item[\"treatment\"] = self.f[\"treatment\"][index]\n","        else:\n","            item[\"treatment\"] = int(self.intervention)\n","        item[\"demog\"] = self.f[\"demog\"][index]\n","        item[\"codes\"] = torch.tensor(\n","            self.pad_bincount(self.f[\"codes\"][self.f[\"code_index\"] == j])\n","        )\n","        item[\"vitals\"] = torch.tensor(\n","            self.f[\"vitals\"][self.f[\"hourly_index\"] == j]\n","        ).float()\n","        item[\"survival\"] = torch.tensor(\n","            self.f[\"survival\"][self.f[\"hourly_index\"] == j]\n","        )\n","        return item\n","\n","    def pad_bincount(self, records):\n","        # get counts of each cod\n","        records = np.bincount(records)\n","        # pad each vector to length T, all possible codes\n","        padded = np.zeros(self.n_codes)\n","        padded[: len(records)] = records\n","        return torch.from_numpy(padded).float()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def padded_collate(batch, pad_index, causal=False):\n","    res = {}\n","    treatment = torch.tensor(np.array([d[\"treatment\"] for d in batch]))\n","    demog = torch.tensor(np.array([d[\"demog\"] for d in batch])).float()\n","    if causal:\n","        res[\"treatment\"] = torch.tensor(np.array([d[\"treatment\"] for d in batch]))\n","        res[\"static\"] = torch.tensor(np.array([d[\"demog\"] for d in batch])).float()\n","    else:\n","        res[\"static\"] = torch.cat([demog, treatment.unsqueeze(1)], 1)\n","    res[\"codes\"] = torch.stack([d[\"codes\"] for d in batch])\n","    res[\"vitals\"] = pad_sequence(\n","        [d[\"vitals\"] for d in batch], batch_first=True, padding_value=pad_index\n","    )\n","    res[\"survival\"] = pad_sequence(\n","        [d[\"survival\"] for d in batch], batch_first=True, padding_value=pad_index\n","    )\n","    return res"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Import the os module\n","import os\n","\n","# Get the current working directory\n","cwd = os.getcwd()\n","\n","# Print the current working directory\n","print(\"Current working directory: {0}\".format(cwd))"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"tsZeYV3Kg5tE"},"source":["#### Install PyTorch lightning"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7431,"status":"ok","timestamp":1681671418692,"user":{"displayName":"Michael Conlin","userId":"06177671838518648738"},"user_tz":420},"id":"VuXIaljfhAmA","outputId":"c594f498-ff72-48f8-d8a5-82c8804b5a15"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pytorch-lightning\n","  Downloading pytorch_lightning-2.0.1.post0-py3-none-any.whl (718 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m718.6/718.6 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning) (2023.4.0)\n","Collecting torchmetrics>=0.7.0\n","  Downloading torchmetrics-0.11.4-py3-none-any.whl (519 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 kB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning) (23.0)\n","Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning) (4.5.0)\n","Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning) (4.65.0)\n","Collecting lightning-utilities>=0.7.0\n","  Downloading lightning_utilities-0.8.0-py3-none-any.whl (20 kB)\n","Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning) (1.22.4)\n","Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning) (2.0.0+cu118)\n","Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning) (6.0)\n","Collecting aiohttp!=4.0.0a0,!=4.0.0a1\n","  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (2.27.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.11.0->pytorch-lightning) (3.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=1.11.0->pytorch-lightning) (3.11.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.11.0->pytorch-lightning) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.11.0->pytorch-lightning) (2.0.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.11.0->pytorch-lightning) (1.11.1)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.11.0->pytorch-lightning) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.11.0->pytorch-lightning) (16.0.1)\n","Collecting async-timeout<5.0,>=4.0.0a3\n","  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n","Collecting yarl<2.0,>=1.0\n","  Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 kB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (22.2.0)\n","Collecting multidict<7.0,>=4.5\n","  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting aiosignal>=1.1.2\n","  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (2.0.12)\n","Collecting frozenlist>=1.1.1\n","  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.11.0->pytorch-lightning) (2.1.2)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (2022.12.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (3.4)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.11.0->pytorch-lightning) (1.3.0)\n","Installing collected packages: multidict, lightning-utilities, frozenlist, async-timeout, yarl, aiosignal, aiohttp, torchmetrics, pytorch-lightning\n","Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 frozenlist-1.3.3 lightning-utilities-0.8.0 multidict-6.0.4 pytorch-lightning-2.0.1.post0 torchmetrics-0.11.4 yarl-1.8.2\n"]}],"source":["%pip install pytorch-lightning"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%pip install pytorch-lightning"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":5207,"status":"ok","timestamp":1681671430643,"user":{"displayName":"Michael Conlin","userId":"06177671838518648738"},"user_tz":420},"id":"Z4i8KhDFe0pW"},"outputs":[],"source":["import pytorch_lightning as pl\n","import torch\n","import math\n","\n","from torch.nn import Linear\n","from src.metric import MeanAbsoluteError, ConcordanceIndex"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":169,"status":"ok","timestamp":1681671979884,"user":{"displayName":"Michael Conlin","userId":"06177671838518648738"},"user_tz":420},"id":"7YLYUTZ2fLn2"},"outputs":[],"source":["class DST(pl.LightningModule):\n","    def __init__(\n","        self,\n","        n_codes,\n","        n_vitals,\n","        n_demog,\n","        d_model,\n","        n_blocks,\n","        n_heads,\n","        dropout,\n","        pad,\n","        dynamic,\n","        causal,\n","        lr=0.001,\n","        alpha=0.01,\n","    ):\n","        super().__init__()\n","        self.save_hyperparameters()\n","        self.embed_codes = Linear(n_codes, d_model)\n","        d1 = 0 if causal else 1\n","        self.embed_static = Linear(d_model + n_demog + d1, d_model)\n","        self.embed_vitals = Linear(n_vitals, d_model)\n","        self.pos_encode = PositionalEncoding(d_model)\n","        self.pad = pad\n","        encoder_layer = torch.nn.TransformerEncoderLayer(\n","            d_model = d_model,\n","            nhead=n_heads,\n","            dropout=dropout,\n","            batch_first=True,\n","            dim_feedforward=d_model*4\n","        )\n","        norm = torch.nn.LayerNorm(d_model)\n","        self.transformer = torch.nn.TransformerEncoder(encoder_layer, n_blocks, norm)\n","        d2 = 1 if causal else 0\n","        self.to_hazard_c = torch.nn.Sequential(\n","            Linear(d_model + d2, d_model//2),\n","            torch.nn.ReLU(),\n","            Linear(d_model//2, 1),\n","            torch.nn.Sigmoid(),\n","        )\n","        self.train_mae = MeanAbsoluteError(pad=pad)\n","        self.val_mae = MeanAbsoluteError(pad=pad)\n","        self.val_ci = ConcordanceIndex(pad=pad)\n","        self.test_mae = MeanAbsoluteError(pad=pad)\n","        self.test_ci = ConcordanceIndex(pad=pad)\n","\n","        # how much to weigh MAE loss\n","        self.alpha = alpha\n","        self.dynamic = dynamic\n","        self.causal = causal\n","\n","\n","    def forward(self, batch):\n","        # static features\n","        x = self.embed_codes(batch[\"codes\"]).unsqueeze(1)\n","        x = self.embed_static(\n","            torch.cat([x, batch[\"static\"].unsqueeze(1)], 2)\n","        )\n","        s = batch[\"vitals\"].shape[1]\n","        # time-varying features\n","        if self.dynamic:\n","            pad_mask = (batch[\"vitals\"][:, :, 0] == self.pad)\n","            x = x + self.embed_vitals(batch[\"vitals\"])\n","            # autoregressive mask\n","            mask = (1 - torch.tril(torch.ones(s, s))).bool().cuda()\n","\n","        else:\n","            mask = None\n","            x = x.repeat(1, s, 1)\n","            pad_mask = (batch[\"vitals\"][:, :, 0] == self.pad)\n","        x = self.pos_encode(x)\n","        x = self.transformer(x, mask, pad_mask)\n","        if self.causal:\n","            t = torch.reshape(batch[\"treatment\"], (-1, 1, 1))\n","            t = t.repeat(1, s, 1)\n","            # concatenate treatment as a new feature\n","            x = torch.cat((x, t), 2).float()\n","        # complement of hazard\n","        q_hat = self.to_hazard_c(x).squeeze(2)\n","        s_hat = q_hat.cumprod(1).clamp(min=1e-8)\n","        return s_hat\n","\n","    def training_step(self, batch, batch_idx):\n","        s_hat =  self(batch)\n","        loss = self.combined_loss(s_hat, batch[\"survival\"])\n","        self.log(\"train_loss\", loss)\n","        self.train_mae(s_hat, batch[\"survival\"])\n","        self.log(\"train_mae\", self.train_mae, on_step=True, on_epoch=False)\n","        return loss\n","\n","\n","\n","    def validation_step(self, batch, batch_idx):\n","        s_hat =  self(batch)\n","        loss = self.combined_loss(s_hat, batch[\"survival\"])\n","        self.val_mae.update(s_hat, batch[\"survival\"])\n","        self.val_ci.update(s_hat, batch[\"survival\"])\n","        self.log(\"val_loss\", loss)\n","        self.log(\"val_mae\", self.val_mae, on_step=True, on_epoch=True)\n","        self.log(\"val_ci\", self.val_ci, on_step=True, on_epoch=True)\n","        return loss\n","\n","\n","    def test_step(self, batch, batch_idx):\n","        s_hat =  self(batch)\n","        loss = self.combined_loss(s_hat, batch[\"survival\"])\n","        self.test_mae.update(s_hat, batch[\"survival\"])\n","        self.test_ci.update(s_hat, batch[\"survival\"])\n","        self.log(\"test_mae\", self.test_mae, on_step=True, on_epoch=True)\n","        self.log(\"test_ci\", self.test_ci, on_step=True, on_epoch=True)\n","        return loss\n","\n","    def predict_step(self, batch, batch_idx):\n","        # returns estimated survival times\n","        s_hat = self(batch)\n","        mask = (batch[\"survival\"] != self.pad)\n","        return (s_hat * mask).sum(1)\n","\n","\n","    def configure_optimizers(self):\n","        return torch.optim.AdamW(self.parameters(), lr=self.hparams.lr)\n","\n","    def ordinal_survival_loss(self, s_hat, y):\n","        # modified cross entropy loss\n","        nlog_survival = -torch.log(s_hat)\n","        nlog_failure = -torch.log(1 - s_hat)\n","        loss = 0\n","        loss += nlog_survival * torch.where(y==self.pad, 0, y)\n","        loss += nlog_failure * torch.where(y==self.pad, 0, (1-y))\n","        return loss.sum() / (y != self.pad).sum()\n","    \n","    def mae_loss(self, s_hat, y):\n","        observed = (y == 0).any(1).int()\n","        t_hat = torch.where(y == self.pad, 0, s_hat).sum(1)\n","        t = torch.where(y == self.pad, 0, y).sum(1)\n","        zeros = torch.zeros(t.shape).cuda()\n","        observed_error = torch.abs(t_hat - t) * observed\n","        censored_error = torch.maximum(zeros, t - t_hat) * (1 - observed)\n","        return (observed_error.sum() + censored_error.sum()) / t.numel()\n","\n","    def combined_loss(self, s_hat, y):\n","        a = self.alpha\n","        ordinal_loss = self.ordinal_survival_loss(s_hat, y)\n","        mae_loss = self.mae_loss(s_hat, y)\n","        return (1 - a) * ordinal_loss + a * mae_loss\n"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":138,"status":"ok","timestamp":1681671986002,"user":{"displayName":"Michael Conlin","userId":"06177671838518648738"},"user_tz":420},"id":"RTV0nv6rfcVq"},"outputs":[],"source":["class PositionalEncoding(torch.nn.Module):\n","\n","    def __init__(self, d_model, dropout=0.1, max_len=5000):\n","        super(PositionalEncoding, self).__init__()\n","        self.dropout = torch.nn.Dropout(p=dropout)\n","\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        x = x + self.pe[:x.size(1), :].unsqueeze(0)\n","        return self.dropout(x)"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":482},"executionInfo":{"elapsed":6572,"status":"ok","timestamp":1681658664189,"user":{"displayName":"Michael Conlin","userId":"06177671838518648738"},"user_tz":420},"id":"h7dqwsY-05j8","outputId":"850e3f4b-cb21-4dcb-ab10-8a2ab21ae287"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting hydra-core\n","  Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from hydra-core) (23.0)\n","Collecting omegaconf<2.4,>=2.2\n","  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting antlr4-python3-runtime==4.9.*\n","  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.9/dist-packages (from omegaconf<2.4,>=2.2->hydra-core) (6.0)\n","Building wheels for collected packages: antlr4-python3-runtime\n","  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144573 sha256=08f1f35e0e32e90c4c268100475a1899e5581006b09ea0cdd763c00dc8afb5aa\n","  Stored in directory: /root/.cache/pip/wheels/23/cf/80/f3efa822e6ab23277902ee9165fe772eeb1dfb8014f359020a\n","Successfully built antlr4-python3-runtime\n","Installing collected packages: antlr4-python3-runtime, omegaconf, hydra-core\n","Successfully installed antlr4-python3-runtime-4.9.3 hydra-core-1.3.2 omegaconf-2.3.0\n"]},{"data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["pydevd_plugins"]}}},"metadata":{},"output_type":"display_data"}],"source":["# !pip install hydra-core --upgrade\n","# only needed if running by command line"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7682,"status":"ok","timestamp":1681678913339,"user":{"displayName":"Michael Conlin","userId":"06177671838518648738"},"user_tz":420},"id":"OOev89fK1MxK","outputId":"657c8dbf-9346-40ad-88d8-aa6f554f9164"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.2/199.2 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["%pip install wandb -qU"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":70},"executionInfo":{"elapsed":5018,"status":"ok","timestamp":1681678934974,"user":{"displayName":"Michael Conlin","userId":"06177671838518648738"},"user_tz":420},"id":"LPFaB18f3_18","outputId":"469673a6-6c6c-4703-d183-c8b1933a0ae2"},"outputs":[{"data":{"application/javascript":"\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    ","text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"text/plain":["True"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["# Log in to your W&B account\n","import wandb\n","wandb.login()"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4681066,"status":"ok","timestamp":1681677449797,"user":{"displayName":"Michael Conlin","userId":"06177671838518648738"},"user_tz":420},"id":"iXQ7dp9xyRjv","outputId":"bd8671a4-b5f8-4f16-9c94-b23d5c6eaa8b"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/dl4h_project/DynST/run.py:12: UserWarning: \n","The version_base parameter is not specified.\n","Please specify a compatability version level, or None.\n","Will assume defaults for version 1.1\n","  @hydra.main(config_path=\".\", config_name=\"config.yaml\")\n","[2023-04-16 19:19:32,399][HYDRA] Launching 6 jobs locally\n","[2023-04-16 19:19:32,399][HYDRA] \t#0 : model.d_model=32 model.alpha=0\n","/usr/local/lib/python3.9/dist-packages/hydra/_internal/core_plugins/basic_launcher.py:74: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n","See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n","  ret = run_job(\n","GPU available: True (cuda), used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","Missing logger folder: /content/drive/MyDrive/dl4h_project/DynST/multirun/2023-04-16/19-19-31/0/0/lightning_logs\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","   | Name         | Type               | Params\n","-----------------------------------------------------\n","0  | embed_codes  | Linear             | 16.6 K\n","1  | embed_static | Linear             | 1.2 K \n","2  | embed_vitals | Linear             | 832   \n","3  | pos_encode   | PositionalEncoding | 0     \n","4  | transformer  | TransformerEncoder | 38.2 K\n","5  | to_hazard_c  | Sequential         | 545   \n","6  | train_mae    | MeanAbsoluteError  | 0     \n","7  | val_mae      | MeanAbsoluteError  | 0     \n","8  | val_ci       | ConcordanceIndex   | 0     \n","9  | test_mae     | MeanAbsoluteError  | 0     \n","10 | test_ci      | ConcordanceIndex   | 0     \n","-----------------------------------------------------\n","57.3 K    Trainable params\n","0         Non-trainable params\n","57.3 K    Total params\n","0.229     Total estimated model params size (MB)\n","2023-04-16 19:19:33.542718: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2023-04-16 19:19:33.600415: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-04-16 19:19:34.574981: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","Sanity Checking: 0it [00:00, ?it/s]/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n","/content/drive/MyDrive/dl4h_project/DynST/src/dataset.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  item[\"codes\"] = torch.tensor(\n","Sanity Checking DataLoader 0:   0% 0/2 [00:00<?, ?it/s]/usr/local/lib/python3.9/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched src_key_padding_mask and src_mask is deprecated. Use same type for both instead.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/torch/nn/modules/transformer.py:544: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n","  return torch._transformer_encoder_layer_fwd(\n","/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n","Epoch 0:   0% 0/664 [00:00<?, ?it/s] /usr/local/lib/python3.9/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n","  warnings.warn(\n","Epoch 0: 100% 664/664 [02:05<00:00,  5.30it/s, v_num=0]\n","Validation: 0it [00:00, ?it/s]\u001b[A\n","Validation:   0% 0/143 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0% 0/143 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0:  14% 20/143 [00:03<00:20,  5.89it/s]\u001b[A\n","Validation DataLoader 0:  28% 40/143 [00:06<00:17,  5.76it/s]\u001b[A\n","Validation DataLoader 0:  42% 60/143 [00:10<00:14,  5.72it/s]\u001b[A\n","Validation DataLoader 0:  56% 80/143 [00:14<00:11,  5.70it/s]\u001b[A\n","Validation DataLoader 0:  70% 100/143 [00:17<00:07,  5.67it/s]\u001b[A\n","Validation DataLoader 0:  84% 120/143 [00:21<00:04,  5.66it/s]\u001b[A\n","Validation DataLoader 0:  98% 140/143 [00:24<00:00,  5.65it/s]\u001b[A\n","Epoch 0: 100% 664/664 [02:30<00:00,  4.41it/s, v_num=0]\n","Epoch 1: 100% 664/664 [02:04<00:00,  5.32it/s, v_num=0]\n","Validation: 0it [00:00, ?it/s]\u001b[A\n","Validation:   0% 0/143 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0% 0/143 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0:  14% 20/143 [00:03<00:20,  5.89it/s]\u001b[A\n","Validation DataLoader 0:  28% 40/143 [00:06<00:17,  5.74it/s]\u001b[A\n","Validation DataLoader 0:  42% 60/143 [00:10<00:14,  5.69it/s]\u001b[A\n","Validation DataLoader 0:  56% 80/143 [00:14<00:11,  5.68it/s]\u001b[A\n","Validation DataLoader 0:  70% 100/143 [00:17<00:07,  5.68it/s]\u001b[A\n","Validation DataLoader 0:  84% 120/143 [00:21<00:04,  5.67it/s]\u001b[A\n","Validation DataLoader 0:  98% 140/143 [00:24<00:00,  5.66it/s]\u001b[A\n","Epoch 1: 100% 664/664 [02:30<00:00,  4.42it/s, v_num=0]\n","Epoch 2: 100% 664/664 [02:05<00:00,  5.30it/s, v_num=0]\n","Validation: 0it [00:00, ?it/s]\u001b[A\n","Validation:   0% 0/143 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0% 0/143 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0:  14% 20/143 [00:03<00:20,  5.93it/s]\u001b[A\n","Validation DataLoader 0:  28% 40/143 [00:06<00:17,  5.79it/s]\u001b[A\n","Validation DataLoader 0:  42% 60/143 [00:10<00:14,  5.70it/s]\u001b[A\n","Validation DataLoader 0:  56% 80/143 [00:14<00:11,  5.68it/s]\u001b[A\n","Validation DataLoader 0:  70% 100/143 [00:17<00:07,  5.67it/s]\u001b[A\n","Validation DataLoader 0:  84% 120/143 [00:21<00:04,  5.65it/s]\u001b[A\n","Validation DataLoader 0:  98% 140/143 [00:24<00:00,  5.64it/s]\u001b[A\n","Epoch 2: 100% 664/664 [02:30<00:00,  4.41it/s, v_num=0]\n","Epoch 3: 100% 664/664 [02:04<00:00,  5.32it/s, v_num=0]\n","Validation: 0it [00:00, ?it/s]\u001b[A\n","Validation:   0% 0/143 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0% 0/143 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0:  14% 20/143 [00:03<00:20,  5.88it/s]\u001b[A\n","Validation DataLoader 0:  28% 40/143 [00:06<00:17,  5.77it/s]\u001b[A\n","Validation DataLoader 0:  42% 60/143 [00:10<00:14,  5.73it/s]\u001b[A\n","Validation DataLoader 0:  56% 80/143 [00:14<00:11,  5.69it/s]\u001b[A\n","Validation DataLoader 0:  70% 100/143 [00:17<00:07,  5.68it/s]\u001b[A\n","Validation DataLoader 0:  84% 120/143 [00:21<00:04,  5.68it/s]\u001b[A\n","Validation DataLoader 0:  98% 140/143 [00:24<00:00,  5.66it/s]\u001b[A\n","Epoch 3: 100% 664/664 [02:30<00:00,  4.42it/s, v_num=0]\n","Epoch 4: 100% 664/664 [02:04<00:00,  5.32it/s, v_num=0]\n","Validation: 0it [00:00, ?it/s]\u001b[A\n","Validation:   0% 0/143 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0% 0/143 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0:  14% 20/143 [00:03<00:20,  5.89it/s]\u001b[A\n","Validation DataLoader 0:  28% 40/143 [00:06<00:17,  5.77it/s]\u001b[A\n","Validation DataLoader 0:  42% 60/143 [00:10<00:14,  5.73it/s]\u001b[A\n","Validation DataLoader 0:  56% 80/143 [00:14<00:11,  5.70it/s]\u001b[A\n","Validation DataLoader 0:  70% 100/143 [00:17<00:07,  5.69it/s]\u001b[A\n","Validation DataLoader 0:  84% 120/143 [00:21<00:04,  5.67it/s]\u001b[A\n","Validation DataLoader 0:  98% 140/143 [00:24<00:00,  5.66it/s]\u001b[A\n","Epoch 4: 100% 664/664 [02:30<00:00,  4.42it/s, v_num=0]\n","Epoch 4: 100% 664/664 [02:30<00:00,  4.42it/s, v_num=0]`Trainer.fit` stopped: `max_epochs=5` reached.\n","Epoch 4: 100% 664/664 [02:30<00:00,  4.42it/s, v_num=0]\n","/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py:148: UserWarning: `.test(ckpt_path=None)` was called without a model. The best model of the previous `fit` call will be used. You can pass `.test(ckpt_path='best')` to use the best model or `.test(ckpt_path='last')` to use the last model. If you pass a value, this warning will be silenced.\n","  rank_zero_warn(\n","You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","Restoring states from the checkpoint path at /content/drive/MyDrive/dl4h_project/DynST/multirun/2023-04-16/19-19-31/0/0/lightning_logs/version_0/checkpoints/epoch=4-step=3320.ckpt\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","Loaded model weights from the checkpoint at /content/drive/MyDrive/dl4h_project/DynST/multirun/2023-04-16/19-19-31/0/0/lightning_logs/version_0/checkpoints/epoch=4-step=3320.ckpt\n","/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n","Testing DataLoader 0: 100% 143/143 [00:24<00:00,  5.72it/s]\n","┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│\u001b[36m \u001b[0m\u001b[36m      test_ci_epoch      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6938294768333435    \u001b[0m\u001b[35m \u001b[0m│\n","│\u001b[36m \u001b[0m\u001b[36m     test_mae_epoch      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   11.709083557128906    \u001b[0m\u001b[35m \u001b[0m│\n","└───────────────────────────┴───────────────────────────┘\n","[2023-04-16 19:32:33,369][HYDRA] \t#1 : model.d_model=32 model.alpha=0.1\n","/usr/local/lib/python3.9/dist-packages/hydra/_internal/core_plugins/basic_launcher.py:74: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n","See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n","  ret = run_job(\n","GPU available: True (cuda), used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","Missing logger folder: /content/drive/MyDrive/dl4h_project/DynST/multirun/2023-04-16/19-19-31/0/1/lightning_logs\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","   | Name         | Type               | Params\n","-----------------------------------------------------\n","0  | embed_codes  | Linear             | 16.6 K\n","1  | embed_static | Linear             | 1.2 K \n","2  | embed_vitals | Linear             | 832   \n","3  | pos_encode   | PositionalEncoding | 0     \n","4  | transformer  | TransformerEncoder | 38.2 K\n","5  | to_hazard_c  | Sequential         | 545   \n","6  | train_mae    | MeanAbsoluteError  | 0     \n","7  | val_mae      | MeanAbsoluteError  | 0     \n","8  | val_ci       | ConcordanceIndex   | 0     \n","9  | test_mae     | MeanAbsoluteError  | 0     \n","10 | test_ci      | ConcordanceIndex   | 0     \n","-----------------------------------------------------\n","57.3 K    Trainable params\n","0         Non-trainable params\n","57.3 K    Total params\n","0.229     Total estimated model params size (MB)\n","Epoch 0: 100% 664/664 [02:05<00:00,  5.31it/s, v_num=0]\n","Validation: 0it [00:00, ?it/s]\u001b[A\n","Validation:   0% 0/143 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0% 0/143 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0:  14% 20/143 [00:03<00:20,  5.92it/s]\u001b[A\n","Validation DataLoader 0:  28% 40/143 [00:06<00:17,  5.76it/s]\u001b[A\n","Validation DataLoader 0:  42% 60/143 [00:10<00:14,  5.71it/s]\u001b[A\n","Validation DataLoader 0:  56% 80/143 [00:14<00:11,  5.70it/s]\u001b[A\n","Validation DataLoader 0:  70% 100/143 [00:17<00:07,  5.69it/s]\u001b[A\n","Validation DataLoader 0:  84% 120/143 [00:21<00:04,  5.68it/s]\u001b[A\n","Validation DataLoader 0:  98% 140/143 [00:24<00:00,  5.66it/s]\u001b[A\n","Epoch 0: 100% 664/664 [02:30<00:00,  4.41it/s, v_num=0]\n","Epoch 1: 100% 664/664 [02:04<00:00,  5.32it/s, v_num=0]\n","Validation: 0it [00:00, ?it/s]\u001b[A\n","Validation:   0% 0/143 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0% 0/143 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0:  14% 20/143 [00:03<00:20,  5.97it/s]\u001b[A\n","Validation DataLoader 0:  28% 40/143 [00:06<00:17,  5.82it/s]\u001b[A\n","Validation DataLoader 0:  42% 60/143 [00:10<00:14,  5.75it/s]\u001b[A\n","Validation DataLoader 0:  56% 80/143 [00:13<00:10,  5.74it/s]\u001b[A\n","Validation DataLoader 0:  70% 100/143 [00:17<00:07,  5.73it/s]\u001b[A\n","Validation DataLoader 0:  84% 120/143 [00:21<00:04,  5.71it/s]\u001b[A\n","Validation DataLoader 0:  98% 140/143 [00:24<00:00,  5.70it/s]\u001b[A\n","Epoch 1: 100% 664/664 [02:29<00:00,  4.43it/s, v_num=0]\n","Epoch 2: 100% 664/664 [02:05<00:00,  5.31it/s, v_num=0]\n","Validation: 0it [00:00, ?it/s]\u001b[A\n","Validation:   0% 0/143 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0% 0/143 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0:  14% 20/143 [00:03<00:20,  5.96it/s]\u001b[A\n","Validation DataLoader 0:  28% 40/143 [00:06<00:17,  5.82it/s]\u001b[A\n","Validation DataLoader 0:  42% 60/143 [00:10<00:14,  5.77it/s]\u001b[A\n","Validation DataLoader 0:  56% 80/143 [00:13<00:10,  5.73it/s]\u001b[A\n","Validation DataLoader 0:  70% 100/143 [00:17<00:07,  5.70it/s]\u001b[A\n","Validation DataLoader 0:  84% 120/143 [00:21<00:04,  5.69it/s]\u001b[A\n","Validation DataLoader 0:  98% 140/143 [00:24<00:00,  5.65it/s]\u001b[A\n","Epoch 2: 100% 664/664 [02:30<00:00,  4.41it/s, v_num=0]\n","Epoch 3: 100% 664/664 [02:04<00:00,  5.32it/s, v_num=0]\n","Validation: 0it [00:00, ?it/s]\u001b[A\n","Validation:   0% 0/143 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0% 0/143 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0:  14% 20/143 [00:03<00:20,  5.92it/s]\u001b[A\n","Validation DataLoader 0:  28% 40/143 [00:06<00:17,  5.80it/s]\u001b[A\n","Validation DataLoader 0:  42% 60/143 [00:10<00:14,  5.76it/s]\u001b[A\n","Validation DataLoader 0:  56% 80/143 [00:13<00:11,  5.72it/s]\u001b[A\n","Validation DataLoader 0:  70% 100/143 [00:17<00:07,  5.70it/s]\u001b[A\n","Validation DataLoader 0:  84% 120/143 [00:21<00:04,  5.69it/s]\u001b[A\n","Validation DataLoader 0:  98% 140/143 [00:24<00:00,  5.69it/s]\u001b[A\n","Epoch 3: 100% 664/664 [02:29<00:00,  4.43it/s, v_num=0]\n","Epoch 4: 100% 664/664 [02:04<00:00,  5.32it/s, v_num=0]\n","Validation: 0it [00:00, ?it/s]\u001b[A\n","Validation:   0% 0/143 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0% 0/143 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0:  14% 20/143 [00:03<00:20,  5.92it/s]\u001b[A\n","Validation DataLoader 0:  28% 40/143 [00:06<00:17,  5.79it/s]\u001b[A\n","Validation DataLoader 0:  42% 60/143 [00:10<00:14,  5.75it/s]\u001b[A\n","Validation DataLoader 0:  56% 80/143 [00:13<00:10,  5.73it/s]\u001b[A\n","Validation DataLoader 0:  70% 100/143 [00:17<00:07,  5.71it/s]\u001b[A\n","Validation DataLoader 0:  84% 120/143 [00:21<00:04,  5.71it/s]\u001b[A\n","Validation DataLoader 0:  98% 140/143 [00:24<00:00,  5.68it/s]\u001b[A\n","Epoch 4: 100% 664/664 [02:30<00:00,  4.42it/s, v_num=0]\n","Epoch 4: 100% 664/664 [02:30<00:00,  4.42it/s, v_num=0]`Trainer.fit` stopped: `max_epochs=5` reached.\n","Epoch 4: 100% 664/664 [02:30<00:00,  4.42it/s, v_num=0]\n","You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","Restoring states from the checkpoint path at /content/drive/MyDrive/dl4h_project/DynST/multirun/2023-04-16/19-19-31/0/1/lightning_logs/version_0/checkpoints/epoch=2-step=1992.ckpt\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","Loaded model weights from the checkpoint at /content/drive/MyDrive/dl4h_project/DynST/multirun/2023-04-16/19-19-31/0/1/lightning_logs/version_0/checkpoints/epoch=2-step=1992.ckpt\n","Testing DataLoader 0: 100% 143/143 [00:24<00:00,  5.75it/s]\n","┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│\u001b[36m \u001b[0m\u001b[36m      test_ci_epoch      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7110733389854431    \u001b[0m\u001b[35m \u001b[0m│\n","│\u001b[36m \u001b[0m\u001b[36m     test_mae_epoch      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   11.352721214294434    \u001b[0m\u001b[35m \u001b[0m│\n","└───────────────────────────┴───────────────────────────┘\n","[2023-04-16 19:45:30,541][HYDRA] \t#2 : model.d_model=32 model.alpha=0.2\n","GPU available: True (cuda), used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","Missing logger folder: /content/drive/MyDrive/dl4h_project/DynST/multirun/2023-04-16/19-19-31/0/2/lightning_logs\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","   | Name         | Type               | Params\n","-----------------------------------------------------\n","0  | embed_codes  | Linear             | 16.6 K\n","1  | embed_static | Linear             | 1.2 K \n","2  | embed_vitals | Linear             | 832   \n","3  | pos_encode   | PositionalEncoding | 0     \n","4  | transformer  | TransformerEncoder | 38.2 K\n","5  | to_hazard_c  | Sequential         | 545   \n","6  | train_mae    | MeanAbsoluteError  | 0     \n","7  | val_mae      | MeanAbsoluteError  | 0     \n","8  | val_ci       | ConcordanceIndex   | 0     \n","9  | test_mae     | MeanAbsoluteError  | 0     \n","10 | test_ci      | ConcordanceIndex   | 0     \n","-----------------------------------------------------\n","57.3 K    Trainable params\n","0         Non-trainable params\n","57.3 K    Total params\n","0.229     Total estimated model params size (MB)\n","Epoch 0: 100% 664/664 [02:04<00:00,  5.31it/s, v_num=0]\n","Validation: 0it [00:00, ?it/s]\u001b[A\n","Validation:   0% 0/143 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0% 0/143 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0:  14% 20/143 [00:03<00:20,  5.95it/s]\u001b[A\n","Validation DataLoader 0:  28% 40/143 [00:06<00:17,  5.77it/s]\u001b[A\n","Validation DataLoader 0:  42% 60/143 [00:10<00:14,  5.72it/s]\u001b[A\n","Validation DataLoader 0:  56% 80/143 [00:14<00:11,  5.71it/s]\u001b[A\n","Validation DataLoader 0:  70% 100/143 [00:17<00:07,  5.70it/s]\u001b[A\n","Validation DataLoader 0:  84% 120/143 [00:21<00:04,  5.68it/s]\u001b[A\n","Validation DataLoader 0:  98% 140/143 [00:24<00:00,  5.68it/s]\u001b[A\n","Epoch 0: 100% 664/664 [02:30<00:00,  4.42it/s, v_num=0]\n","Epoch 1: 100% 664/664 [02:04<00:00,  5.33it/s, v_num=0]\n","Validation: 0it [00:00, ?it/s]\u001b[A\n","Validation:   0% 0/143 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0% 0/143 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0:  14% 20/143 [00:03<00:20,  5.87it/s]\u001b[A\n","Validation DataLoader 0:  28% 40/143 [00:06<00:17,  5.73it/s]\u001b[A\n","Validation DataLoader 0:  42% 60/143 [00:10<00:14,  5.67it/s]\u001b[A\n","Validation DataLoader 0:  56% 80/143 [00:14<00:11,  5.67it/s]\u001b[A\n","Validation DataLoader 0:  70% 100/143 [00:17<00:07,  5.67it/s]\u001b[A\n","Validation DataLoader 0:  84% 120/143 [00:21<00:04,  5.66it/s]\u001b[A\n","Validation DataLoader 0:  98% 140/143 [00:24<00:00,  5.66it/s]\u001b[A\n","Epoch 1: 100% 664/664 [02:29<00:00,  4.43it/s, v_num=0]\n","Epoch 2: 100% 664/664 [02:04<00:00,  5.33it/s, v_num=0]\n","Validation: 0it [00:00, ?it/s]\u001b[A\n","Validation:   0% 0/143 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0% 0/143 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0:  14% 20/143 [00:03<00:20,  5.86it/s]\u001b[A\n","Validation DataLoader 0:  28% 40/143 [00:06<00:17,  5.77it/s]\u001b[A\n","Validation DataLoader 0:  42% 60/143 [00:10<00:14,  5.74it/s]\u001b[A\n","Validation DataLoader 0:  56% 80/143 [00:14<00:11,  5.70it/s]\u001b[A\n","Validation DataLoader 0:  70% 100/143 [00:17<00:07,  5.69it/s]\u001b[A\n","Validation DataLoader 0:  84% 120/143 [00:21<00:04,  5.69it/s]\u001b[A\n","Validation DataLoader 0:  98% 140/143 [00:24<00:00,  5.67it/s]\u001b[A\n","Epoch 2: 100% 664/664 [02:29<00:00,  4.43it/s, v_num=0]\n","Epoch 3: 100% 664/664 [02:04<00:00,  5.33it/s, v_num=0]\n","Validation: 0it [00:00, ?it/s]\u001b[A\n","Validation:   0% 0/143 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0% 0/143 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0:  14% 20/143 [00:03<00:20,  5.89it/s]\u001b[A\n","Validation DataLoader 0:  28% 40/143 [00:06<00:18,  5.72it/s]\u001b[A\n","Validation DataLoader 0:  42% 60/143 [00:10<00:14,  5.68it/s]\u001b[A\n","Validation DataLoader 0:  56% 80/143 [00:14<00:11,  5.66it/s]\u001b[A\n","Validation DataLoader 0:  70% 100/143 [00:17<00:07,  5.65it/s]\u001b[A\n","Validation DataLoader 0:  84% 120/143 [00:21<00:04,  5.64it/s]\u001b[A\n","Validation DataLoader 0:  98% 140/143 [00:24<00:00,  5.65it/s]\u001b[A\n","Epoch 3: 100% 664/664 [02:29<00:00,  4.43it/s, v_num=0]\n","Epoch 4: 100% 664/664 [02:04<00:00,  5.33it/s, v_num=0]\n","Validation: 0it [00:00, ?it/s]\u001b[A\n","Validation:   0% 0/143 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0% 0/143 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0:  14% 20/143 [00:03<00:20,  5.95it/s]\u001b[A\n","Validation DataLoader 0:  28% 40/143 [00:06<00:17,  5.77it/s]\u001b[A\n","Validation DataLoader 0:  42% 60/143 [00:10<00:14,  5.72it/s]\u001b[A\n","Validation DataLoader 0:  56% 80/143 [00:14<00:11,  5.70it/s]\u001b[A\n","Validation DataLoader 0:  70% 100/143 [00:17<00:07,  5.70it/s]\u001b[A\n","Validation DataLoader 0:  84% 120/143 [00:21<00:04,  5.68it/s]\u001b[A\n","Validation DataLoader 0:  98% 140/143 [00:24<00:00,  5.68it/s]\u001b[A\n","Epoch 4: 100% 664/664 [02:29<00:00,  4.43it/s, v_num=0]\n","Epoch 4: 100% 664/664 [02:29<00:00,  4.43it/s, v_num=0]`Trainer.fit` stopped: `max_epochs=5` reached.\n","Epoch 4: 100% 664/664 [02:29<00:00,  4.43it/s, v_num=0]\n","You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","Restoring states from the checkpoint path at /content/drive/MyDrive/dl4h_project/DynST/multirun/2023-04-16/19-19-31/0/2/lightning_logs/version_0/checkpoints/epoch=2-step=1992.ckpt\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","Loaded model weights from the checkpoint at /content/drive/MyDrive/dl4h_project/DynST/multirun/2023-04-16/19-19-31/0/2/lightning_logs/version_0/checkpoints/epoch=2-step=1992.ckpt\n","Testing DataLoader 0: 100% 143/143 [00:24<00:00,  5.72it/s]\n","┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│\u001b[36m \u001b[0m\u001b[36m      test_ci_epoch      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7144995331764221    \u001b[0m\u001b[35m \u001b[0m│\n","│\u001b[36m \u001b[0m\u001b[36m     test_mae_epoch      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   11.175790786743164    \u001b[0m\u001b[35m \u001b[0m│\n","└───────────────────────────┴───────────────────────────┘\n","[2023-04-16 19:58:26,507][HYDRA] \t#3 : model.d_model=64 model.alpha=0\n","GPU available: True (cuda), used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","Missing logger folder: /content/drive/MyDrive/dl4h_project/DynST/multirun/2023-04-16/19-19-31/0/3/lightning_logs\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","   | Name         | Type               | Params\n","-----------------------------------------------------\n","0  | embed_codes  | Linear             | 33.2 K\n","1  | embed_static | Linear             | 4.4 K \n","2  | embed_vitals | Linear             | 1.7 K \n","3  | pos_encode   | PositionalEncoding | 0     \n","4  | transformer  | TransformerEncoder | 150 K \n","5  | to_hazard_c  | Sequential         | 2.1 K \n","6  | train_mae    | MeanAbsoluteError  | 0     \n","7  | val_mae      | MeanAbsoluteError  | 0     \n","8  | val_ci       | ConcordanceIndex   | 0     \n","9  | test_mae     | MeanAbsoluteError  | 0     \n","10 | test_ci      | ConcordanceIndex   | 0     \n","-----------------------------------------------------\n","191 K     Trainable params\n","0         Non-trainable params\n","191 K     Total params\n","0.765     Total estimated model params size (MB)\n","Epoch 0: 100% 664/664 [02:05<00:00,  5.30it/s, v_num=0]\n","Validation: 0it [00:00, ?it/s]\u001b[A\n","Validation:   0% 0/143 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0% 0/143 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0:  14% 20/143 [00:03<00:20,  5.95it/s]\u001b[A\n","Validation DataLoader 0:  28% 40/143 [00:06<00:17,  5.79it/s]\u001b[A\n","Validation DataLoader 0:  42% 60/143 [00:10<00:14,  5.72it/s]\u001b[A\n","Validation DataLoader 0:  56% 80/143 [00:14<00:11,  5.71it/s]\u001b[A\n","Validation DataLoader 0:  70% 100/143 [00:17<00:07,  5.70it/s]\u001b[A\n","Validation DataLoader 0:  84% 120/143 [00:21<00:04,  5.69it/s]\u001b[A\n","Validation DataLoader 0:  98% 140/143 [00:24<00:00,  5.68it/s]\u001b[A\n","Epoch 0: 100% 664/664 [02:30<00:00,  4.41it/s, v_num=0]\n","Epoch 1: 100% 664/664 [02:05<00:00,  5.31it/s, v_num=0]\n","Validation: 0it [00:00, ?it/s]\u001b[A\n","Validation:   0% 0/143 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0% 0/143 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0:  14% 20/143 [00:03<00:20,  5.89it/s]\u001b[A\n","Validation DataLoader 0:  28% 40/143 [00:06<00:17,  5.74it/s]\u001b[A\n","Validation DataLoader 0:  42% 60/143 [00:10<00:14,  5.69it/s]\u001b[A\n","Validation DataLoader 0:  56% 80/143 [00:14<00:11,  5.65it/s]\u001b[A\n","Validation DataLoader 0:  70% 100/143 [00:17<00:07,  5.65it/s]\u001b[A\n","Validation DataLoader 0:  84% 120/143 [00:21<00:04,  5.65it/s]\u001b[A\n","Validation DataLoader 0:  98% 140/143 [00:24<00:00,  5.64it/s]\u001b[A\n","Epoch 1: 100% 664/664 [02:30<00:00,  4.41it/s, v_num=0]\n","Epoch 2: 100% 664/664 [02:04<00:00,  5.31it/s, v_num=0]\n","Validation: 0it [00:00, ?it/s]\u001b[A\n","Validation:   0% 0/143 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0% 0/143 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0:  14% 20/143 [00:03<00:21,  5.86it/s]\u001b[A\n","Validation DataLoader 0:  28% 40/143 [00:06<00:17,  5.75it/s]\u001b[A\n","Validation DataLoader 0:  42% 60/143 [00:10<00:14,  5.70it/s]\u001b[A\n","Validation DataLoader 0:  56% 80/143 [00:14<00:11,  5.68it/s]\u001b[A\n","Validation DataLoader 0:  70% 100/143 [00:17<00:07,  5.67it/s]\u001b[A\n","Validation DataLoader 0:  84% 120/143 [00:21<00:04,  5.66it/s]\u001b[A\n","Validation DataLoader 0:  98% 140/143 [00:24<00:00,  5.66it/s]\u001b[A\n","Epoch 2: 100% 664/664 [02:30<00:00,  4.42it/s, v_num=0]\n","Epoch 3: 100% 664/664 [02:05<00:00,  5.30it/s, v_num=0]\n","Validation: 0it [00:00, ?it/s]\u001b[A\n","Validation:   0% 0/143 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0% 0/143 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0:  14% 20/143 [00:03<00:20,  5.93it/s]\u001b[A\n","Validation DataLoader 0:  28% 40/143 [00:06<00:17,  5.77it/s]\u001b[A\n","Validation DataLoader 0:  42% 60/143 [00:10<00:14,  5.73it/s]\u001b[A\n","Validation DataLoader 0:  56% 80/143 [00:14<00:11,  5.70it/s]\u001b[A\n","Validation DataLoader 0:  70% 100/143 [00:17<00:07,  5.68it/s]\u001b[A\n","Validation DataLoader 0:  84% 120/143 [00:21<00:04,  5.67it/s]\u001b[A\n","Validation DataLoader 0:  98% 140/143 [00:24<00:00,  5.66it/s]\u001b[A\n","Epoch 3: 100% 664/664 [02:30<00:00,  4.41it/s, v_num=0]\n","Epoch 4: 100% 664/664 [02:05<00:00,  5.30it/s, v_num=0]\n","Validation: 0it [00:00, ?it/s]\u001b[A\n","Validation:   0% 0/143 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0% 0/143 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0:  14% 20/143 [00:03<00:20,  5.88it/s]\u001b[A\n","Validation DataLoader 0:  28% 40/143 [00:06<00:18,  5.72it/s]\u001b[A\n","Validation DataLoader 0:  42% 60/143 [00:10<00:14,  5.67it/s]\u001b[A\n","Validation DataLoader 0:  56% 80/143 [00:14<00:11,  5.66it/s]\u001b[A\n","Validation DataLoader 0:  70% 100/143 [00:17<00:07,  5.65it/s]\u001b[A\n","Validation DataLoader 0:  84% 120/143 [00:21<00:04,  5.64it/s]\u001b[A\n","Validation DataLoader 0:  98% 140/143 [00:24<00:00,  5.63it/s]\u001b[A\n","Epoch 4: 100% 664/664 [02:30<00:00,  4.41it/s, v_num=0]\n","Epoch 4: 100% 664/664 [02:30<00:00,  4.41it/s, v_num=0]`Trainer.fit` stopped: `max_epochs=5` reached.\n","Epoch 4: 100% 664/664 [02:30<00:00,  4.41it/s, v_num=0]\n","You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","Restoring states from the checkpoint path at /content/drive/MyDrive/dl4h_project/DynST/multirun/2023-04-16/19-19-31/0/3/lightning_logs/version_0/checkpoints/epoch=3-step=2656.ckpt\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","Loaded model weights from the checkpoint at /content/drive/MyDrive/dl4h_project/DynST/multirun/2023-04-16/19-19-31/0/3/lightning_logs/version_0/checkpoints/epoch=3-step=2656.ckpt\n","Testing DataLoader 0: 100% 143/143 [00:25<00:00,  5.70it/s]\n","┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│\u001b[36m \u001b[0m\u001b[36m      test_ci_epoch      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6872921586036682    \u001b[0m\u001b[35m \u001b[0m│\n","│\u001b[36m \u001b[0m\u001b[36m     test_mae_epoch      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    11.98781967163086    \u001b[0m\u001b[35m \u001b[0m│\n","└───────────────────────────┴───────────────────────────┘\n","[2023-04-16 20:11:25,334][HYDRA] \t#4 : model.d_model=64 model.alpha=0.1\n","GPU available: True (cuda), used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","Missing logger folder: /content/drive/MyDrive/dl4h_project/DynST/multirun/2023-04-16/19-19-31/0/4/lightning_logs\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","   | Name         | Type               | Params\n","-----------------------------------------------------\n","0  | embed_codes  | Linear             | 33.2 K\n","1  | embed_static | Linear             | 4.4 K \n","2  | embed_vitals | Linear             | 1.7 K \n","3  | pos_encode   | PositionalEncoding | 0     \n","4  | transformer  | TransformerEncoder | 150 K \n","5  | to_hazard_c  | Sequential         | 2.1 K \n","6  | train_mae    | MeanAbsoluteError  | 0     \n","7  | val_mae      | MeanAbsoluteError  | 0     \n","8  | val_ci       | ConcordanceIndex   | 0     \n","9  | test_mae     | MeanAbsoluteError  | 0     \n","10 | test_ci      | ConcordanceIndex   | 0     \n","-----------------------------------------------------\n","191 K     Trainable params\n","0         Non-trainable params\n","191 K     Total params\n","0.765     Total estimated model params size (MB)\n","Epoch 0: 100% 664/664 [02:05<00:00,  5.31it/s, v_num=0]\n","Validation: 0it [00:00, ?it/s]\u001b[A\n","Validation:   0% 0/143 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0% 0/143 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0:  14% 20/143 [00:03<00:20,  5.95it/s]\u001b[A\n","Validation DataLoader 0:  28% 40/143 [00:06<00:17,  5.80it/s]\u001b[A\n","Validation DataLoader 0:  42% 60/143 [00:10<00:14,  5.73it/s]\u001b[A\n","Validation DataLoader 0:  56% 80/143 [00:14<00:11,  5.71it/s]\u001b[A\n","Validation DataLoader 0:  70% 100/143 [00:17<00:07,  5.70it/s]\u001b[A\n","Validation DataLoader 0:  84% 120/143 [00:21<00:04,  5.68it/s]\u001b[A\n","Validation DataLoader 0:  98% 140/143 [00:24<00:00,  5.67it/s]\u001b[A\n","Epoch 0: 100% 664/664 [02:30<00:00,  4.41it/s, v_num=0]\n","Epoch 1: 100% 664/664 [02:05<00:00,  5.31it/s, v_num=0]\n","Validation: 0it [00:00, ?it/s]\u001b[A\n","Validation:   0% 0/143 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0% 0/143 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0:  14% 20/143 [00:03<00:20,  5.86it/s]\u001b[A\n","Validation DataLoader 0:  28% 40/143 [00:06<00:17,  5.75it/s]\u001b[A\n","Validation DataLoader 0:  42% 60/143 [00:10<00:14,  5.70it/s]\u001b[A\n","Validation DataLoader 0:  56% 80/143 [00:14<00:11,  5.68it/s]\u001b[A\n","Validation DataLoader 0:  70% 100/143 [00:17<00:07,  5.67it/s]\u001b[A\n","Validation DataLoader 0:  84% 120/143 [00:21<00:04,  5.67it/s]\u001b[A\n","Validation DataLoader 0:  98% 140/143 [00:24<00:00,  5.66it/s]\u001b[A\n","Epoch 1: 100% 664/664 [02:30<00:00,  4.41it/s, v_num=0]\n","Epoch 2: 100% 664/664 [02:05<00:00,  5.29it/s, v_num=0]\n","Validation: 0it [00:00, ?it/s]\u001b[A\n","Validation:   0% 0/143 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0% 0/143 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0:  14% 20/143 [00:03<00:20,  5.90it/s]\u001b[A\n","Validation DataLoader 0:  28% 40/143 [00:06<00:17,  5.75it/s]\u001b[A\n","Validation DataLoader 0:  42% 60/143 [00:10<00:14,  5.71it/s]\u001b[A\n","Validation DataLoader 0:  56% 80/143 [00:14<00:11,  5.68it/s]\u001b[A\n","Validation DataLoader 0:  70% 100/143 [00:17<00:07,  5.65it/s]\u001b[A\n","Validation DataLoader 0:  84% 120/143 [00:21<00:04,  5.64it/s]\u001b[A\n","Validation DataLoader 0:  98% 140/143 [00:24<00:00,  5.64it/s]\u001b[A\n","Epoch 2: 100% 664/664 [02:30<00:00,  4.40it/s, v_num=0]\n","Epoch 3: 100% 664/664 [02:05<00:00,  5.29it/s, v_num=0]\n","Validation: 0it [00:00, ?it/s]\u001b[A\n","Validation:   0% 0/143 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0% 0/143 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0:  14% 20/143 [00:03<00:20,  5.88it/s]\u001b[A\n","Validation DataLoader 0:  28% 40/143 [00:06<00:17,  5.72it/s]\u001b[A\n","Validation DataLoader 0:  42% 60/143 [00:10<00:14,  5.68it/s]\u001b[A\n","Validation DataLoader 0:  56% 80/143 [00:14<00:11,  5.66it/s]\u001b[A\n","Validation DataLoader 0:  70% 100/143 [00:17<00:07,  5.63it/s]\u001b[A\n","Validation DataLoader 0:  84% 120/143 [00:21<00:04,  5.63it/s]\u001b[A\n","Validation DataLoader 0:  98% 140/143 [00:24<00:00,  5.64it/s]\u001b[A\n","Epoch 3: 100% 664/664 [02:30<00:00,  4.40it/s, v_num=0]\n","Epoch 4: 100% 664/664 [02:05<00:00,  5.30it/s, v_num=0]\n","Validation: 0it [00:00, ?it/s]\u001b[A\n","Validation:   0% 0/143 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0% 0/143 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0:  14% 20/143 [00:03<00:20,  5.95it/s]\u001b[A\n","Validation DataLoader 0:  28% 40/143 [00:06<00:17,  5.75it/s]\u001b[A\n","Validation DataLoader 0:  42% 60/143 [00:10<00:14,  5.72it/s]\u001b[A\n","Validation DataLoader 0:  56% 80/143 [00:14<00:11,  5.70it/s]\u001b[A\n","Validation DataLoader 0:  70% 100/143 [00:17<00:07,  5.69it/s]\u001b[A\n","Validation DataLoader 0:  84% 120/143 [00:21<00:04,  5.68it/s]\u001b[A\n","Validation DataLoader 0:  98% 140/143 [00:24<00:00,  5.68it/s]\u001b[A\n","Epoch 4: 100% 664/664 [02:30<00:00,  4.41it/s, v_num=0]\n","Epoch 4: 100% 664/664 [02:30<00:00,  4.41it/s, v_num=0]`Trainer.fit` stopped: `max_epochs=5` reached.\n","Epoch 4: 100% 664/664 [02:30<00:00,  4.41it/s, v_num=0]\n","You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","Restoring states from the checkpoint path at /content/drive/MyDrive/dl4h_project/DynST/multirun/2023-04-16/19-19-31/0/4/lightning_logs/version_0/checkpoints/epoch=4-step=3320.ckpt\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","Loaded model weights from the checkpoint at /content/drive/MyDrive/dl4h_project/DynST/multirun/2023-04-16/19-19-31/0/4/lightning_logs/version_0/checkpoints/epoch=4-step=3320.ckpt\n","Testing DataLoader 0: 100% 143/143 [00:25<00:00,  5.72it/s]\n","┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│\u001b[36m \u001b[0m\u001b[36m      test_ci_epoch      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7002747654914856    \u001b[0m\u001b[35m \u001b[0m│\n","│\u001b[36m \u001b[0m\u001b[36m     test_mae_epoch      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   11.451096534729004    \u001b[0m\u001b[35m \u001b[0m│\n","└───────────────────────────┴───────────────────────────┘\n","[2023-04-16 20:24:24,849][HYDRA] \t#5 : model.d_model=64 model.alpha=0.2\n","GPU available: True (cuda), used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","Missing logger folder: /content/drive/MyDrive/dl4h_project/DynST/multirun/2023-04-16/19-19-31/0/5/lightning_logs\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","   | Name         | Type               | Params\n","-----------------------------------------------------\n","0  | embed_codes  | Linear             | 33.2 K\n","1  | embed_static | Linear             | 4.4 K \n","2  | embed_vitals | Linear             | 1.7 K \n","3  | pos_encode   | PositionalEncoding | 0     \n","4  | transformer  | TransformerEncoder | 150 K \n","5  | to_hazard_c  | Sequential         | 2.1 K \n","6  | train_mae    | MeanAbsoluteError  | 0     \n","7  | val_mae      | MeanAbsoluteError  | 0     \n","8  | val_ci       | ConcordanceIndex   | 0     \n","9  | test_mae     | MeanAbsoluteError  | 0     \n","10 | test_ci      | ConcordanceIndex   | 0     \n","-----------------------------------------------------\n","191 K     Trainable params\n","0         Non-trainable params\n","191 K     Total params\n","0.765     Total estimated model params size (MB)\n","Epoch 0: 100% 664/664 [02:06<00:00,  5.23it/s, v_num=0]\n","Validation: 0it [00:00, ?it/s]\u001b[A\n","Validation:   0% 0/143 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0% 0/143 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0:  14% 20/143 [00:03<00:21,  5.77it/s]\u001b[A\n","Validation DataLoader 0:  28% 40/143 [00:07<00:18,  5.61it/s]\u001b[A\n","Validation DataLoader 0:  42% 60/143 [00:10<00:14,  5.59it/s]\u001b[A\n","Validation DataLoader 0:  56% 80/143 [00:14<00:11,  5.57it/s]\u001b[A\n","Validation DataLoader 0:  70% 100/143 [00:18<00:07,  5.55it/s]\u001b[A\n","Validation DataLoader 0:  84% 120/143 [00:21<00:04,  5.55it/s]\u001b[A\n","Validation DataLoader 0:  98% 140/143 [00:25<00:00,  5.56it/s]\u001b[A\n","Epoch 0: 100% 664/664 [02:32<00:00,  4.35it/s, v_num=0]\n","Epoch 1: 100% 664/664 [02:06<00:00,  5.26it/s, v_num=0]\n","Validation: 0it [00:00, ?it/s]\u001b[A\n","Validation:   0% 0/143 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0% 0/143 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0:  14% 20/143 [00:03<00:20,  5.91it/s]\u001b[A\n","Validation DataLoader 0:  28% 40/143 [00:06<00:17,  5.77it/s]\u001b[A\n","Validation DataLoader 0:  42% 60/143 [00:10<00:14,  5.70it/s]\u001b[A\n","Validation DataLoader 0:  56% 80/143 [00:14<00:11,  5.68it/s]\u001b[A\n","Validation DataLoader 0:  70% 100/143 [00:17<00:07,  5.67it/s]\u001b[A\n","Validation DataLoader 0:  84% 120/143 [00:21<00:04,  5.65it/s]\u001b[A\n","Validation DataLoader 0:  98% 140/143 [00:24<00:00,  5.64it/s]\u001b[A\n","Epoch 1: 100% 664/664 [02:31<00:00,  4.38it/s, v_num=0]\n","Epoch 2: 100% 664/664 [02:05<00:00,  5.28it/s, v_num=0]\n","Validation: 0it [00:00, ?it/s]\u001b[A\n","Validation:   0% 0/143 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0% 0/143 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0:  14% 20/143 [00:03<00:20,  5.90it/s]\u001b[A\n","Validation DataLoader 0:  28% 40/143 [00:06<00:17,  5.77it/s]\u001b[A\n","Validation DataLoader 0:  42% 60/143 [00:10<00:14,  5.72it/s]\u001b[A\n","Validation DataLoader 0:  56% 80/143 [00:14<00:11,  5.69it/s]\u001b[A\n","Validation DataLoader 0:  70% 100/143 [00:17<00:07,  5.68it/s]\u001b[A\n","Validation DataLoader 0:  84% 120/143 [00:21<00:04,  5.67it/s]\u001b[A\n","Validation DataLoader 0:  98% 140/143 [00:24<00:00,  5.65it/s]\u001b[A\n","Epoch 2: 100% 664/664 [02:31<00:00,  4.40it/s, v_num=0]\n","Epoch 3: 100% 664/664 [02:05<00:00,  5.28it/s, v_num=0]\n","Validation: 0it [00:00, ?it/s]\u001b[A\n","Validation:   0% 0/143 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0% 0/143 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0:  14% 20/143 [00:03<00:20,  5.95it/s]\u001b[A\n","Validation DataLoader 0:  28% 40/143 [00:06<00:17,  5.79it/s]\u001b[A\n","Validation DataLoader 0:  42% 60/143 [00:10<00:14,  5.72it/s]\u001b[A\n","Validation DataLoader 0:  56% 80/143 [00:14<00:11,  5.69it/s]\u001b[A\n","Validation DataLoader 0:  70% 100/143 [00:17<00:07,  5.68it/s]\u001b[A\n","Validation DataLoader 0:  84% 120/143 [00:21<00:04,  5.66it/s]\u001b[A\n","Validation DataLoader 0:  98% 140/143 [00:24<00:00,  5.63it/s]\u001b[A\n","Epoch 3: 100% 664/664 [02:31<00:00,  4.39it/s, v_num=0]\n","Epoch 4: 100% 664/664 [02:05<00:00,  5.30it/s, v_num=0]\n","Validation: 0it [00:00, ?it/s]\u001b[A\n","Validation:   0% 0/143 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0% 0/143 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0:  14% 20/143 [00:03<00:20,  5.88it/s]\u001b[A\n","Validation DataLoader 0:  28% 40/143 [00:06<00:17,  5.76it/s]\u001b[A\n","Validation DataLoader 0:  42% 60/143 [00:10<00:14,  5.72it/s]\u001b[A\n","Validation DataLoader 0:  56% 80/143 [00:14<00:11,  5.70it/s]\u001b[A\n","Validation DataLoader 0:  70% 100/143 [00:17<00:07,  5.68it/s]\u001b[A\n","Validation DataLoader 0:  84% 120/143 [00:21<00:04,  5.67it/s]\u001b[A\n","Validation DataLoader 0:  98% 140/143 [00:24<00:00,  5.65it/s]\u001b[A\n","Epoch 4: 100% 664/664 [02:30<00:00,  4.41it/s, v_num=0]\n","Epoch 4: 100% 664/664 [02:30<00:00,  4.41it/s, v_num=0]`Trainer.fit` stopped: `max_epochs=5` reached.\n","Epoch 4: 100% 664/664 [02:30<00:00,  4.41it/s, v_num=0]\n","You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","Restoring states from the checkpoint path at /content/drive/MyDrive/dl4h_project/DynST/multirun/2023-04-16/19-19-31/0/5/lightning_logs/version_0/checkpoints/epoch=2-step=1992.ckpt\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","Loaded model weights from the checkpoint at /content/drive/MyDrive/dl4h_project/DynST/multirun/2023-04-16/19-19-31/0/5/lightning_logs/version_0/checkpoints/epoch=2-step=1992.ckpt\n","Testing DataLoader 0: 100% 143/143 [00:25<00:00,  5.72it/s]\n","┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│\u001b[36m \u001b[0m\u001b[36m      test_ci_epoch      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7157701849937439    \u001b[0m\u001b[35m \u001b[0m│\n","│\u001b[36m \u001b[0m\u001b[36m     test_mae_epoch      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   11.172209739685059    \u001b[0m\u001b[35m \u001b[0m│\n","└───────────────────────────┴───────────────────────────┘\n"]}],"source":["# !python run.py -m model.d_model=32,64 model.alpha=0,0.1,0.2\n","# how to run the model from the command line using hydra, and multirun"]},{"cell_type":"markdown","metadata":{"id":"2aa2Jt-A734N"},"source":["# New Section"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","from torch.utils.data import DataLoader, random_split\n","import pytorch_lightning as pl\n","from pytorch_lightning.callbacks import ModelCheckpoint\n","\n","# set the default parameters from the config.yaml file\n","# preprocess\n","preprocess_seed = 30\n","\n","# train\n","train_frac = .7\n","val_frac = .15\n","accelerator = \"gpu\"\n","devices = 1\n","max_epochs = 5\n","batch_size = 32\n","train_seed = 0\n","\n","causal = False\n","\n","# model\n","_target_ = \"src.model.DST\"\n","d_model = 32\n","n_blocks = 3\n","n_heads = 8\n","dropout = .1\n","pad = -100\n","dynamic = True\n","lr = 0.001\n","alpha = 0.01\n","\n","pipeline = Mimic3Pipeline(work_dir=cwd, seed=preprocess_seed)\n","pipeline.run()\n","dataset = Mimic3Dataset(work_dir=cwd, seed=preprocess_seed)\n","\n","train_size = int(train_frac * len(dataset))\n","if train_frac + val_frac == 1.0:\n","    val_size = len(dataset) - train_size\n","    test_size = 0\n","else:\n","    val_size = int(val_frac * len(dataset))\n","    test_size = len(dataset) - train_size - val_size\n","train_set, val_set, test_set = random_split(\n","    dataset,\n","    (train_size, val_size, test_size),\n","    torch.Generator().manual_seed(train_seed)\n",")\n","\n","###### new code\n","\n","def collate(x):\n","        return padded_collate(x, pad_index=pad, causal=causal)\n","\n","    train_loader = DataLoader(\n","        train_set,\n","        collate_fn=collate,\n","        batch_size=batch_size,\n","        shuffle=True\n","    )\n","    val_loader = DataLoader(\n","        val_set, collate_fn=collate, batch_size=batch_size\n","    )\n","    if test_size:\n","        test_loader = DataLoader(\n","            test_set, collate_fn=collate, batch_size=batch_size\n","        )\n","    model = instantiate(\n","        cfg.model, n_codes=dataset.n_codes, n_demog=dataset.n_demog,\n","        n_vitals=dataset.n_vitals, causal=cfg.causal,\n","    )\n","    callbacks = [ModelCheckpoint(monitor=\"val_mae_epoch\", mode=\"min\")]\n","    trainer = pl.Trainer(\n","        accelerator=cfg.train.accelerator,\n","        devices=cfg.train.devices,\n","        max_epochs=cfg.train.max_epochs,\n","        callbacks=callbacks,\n","    )\n","    trainer.fit(model, train_loader, val_loader)\n","    if test_size:\n","        trainer.test(dataloaders=test_loader)"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyM4f4HqebxS086cY4L2R9hZ","provenance":[]},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
