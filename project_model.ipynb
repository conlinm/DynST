{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### This is the Primary Notebook for the Project\n","If you have not run the preprocessing notebook, please do so before running this notebook. This notebook will perform the following tasks:\n","- Load the preprocessed data\n","- Define the Mimic3Dataset class to create a PyTorch Dataset\n","- Define the DST model"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"HhXLWyabgZZR"},"source":[" #### Mount Google Drive and change directory to the project folder"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1092,"status":"ok","timestamp":1681671400544,"user":{"displayName":"Michael Conlin","userId":"06177671838518648738"},"user_tz":420},"id":"zdluUkf7gjiw","outputId":"95abac03-747f-44a1-ca3a-e5648eff990d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/dl4h_project/DynST/\n","%ls"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Mimic3Dataset Class\n","This will be called later to create the dataset for training and testing the model. It will read the csv file created by the Mimic3Pipeline class, and create the dataset for training and testing the model.\n","Also provided are the functions to pad and collate the data for use by the model."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","import numpy as np\n","import torch\n","from torch.utils.data import Dataset\n","from torch.nn.utils.rnn import pad_sequence"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class Mimic3Dataset(Dataset):\n","    def __init__(self, work_dir, seed, intervention=None):\n","        fdir = f\"{work_dir}/data/preprocessed_{seed}\"\n","        self.f = {}\n","        for fname in os.listdir(fdir):\n","            if fname.endswith(\".npy\"):\n","                self.f[fname[:-4]] = np.load(\n","                    f\"{fdir}/{fname}\", allow_pickle=True\n","                    )\n","        self.ix = self.f[\"patient_index\"]\n","        self.code_lookup = np.insert(self.f[\"code_lookup\"], 0, \"pad\")\n","        self.codes = self.f[\"codes\"] + 1\n","        self.n_codes = len(self.code_lookup)\n","        self.n_vitals = self.f[\"vitals\"].shape[1]\n","        self.n_demog = self.f[\"demog\"].shape[1]\n","        self.pad_value = - 100\n","        # if supplied, represents treatment (True) or control (False)\n","        self.intervention = intervention\n","\n","\n","\n","    def __len__(self):\n","        return len(self.f[\"treatment\"])\n","\n","    def __getitem__(self, index):\n","        item = {}\n","        j = self.ix[index]\n","        if self.intervention is None:\n","            item[\"treatment\"] = self.f[\"treatment\"][index]\n","        else:\n","            item[\"treatment\"] = int(self.intervention)\n","        item[\"demog\"] = self.f[\"demog\"][index]\n","        item[\"codes\"] = torch.tensor(\n","            self.pad_bincount(self.f[\"codes\"][self.f[\"code_index\"] == j])\n","        )\n","        item[\"vitals\"] = torch.tensor(\n","            self.f[\"vitals\"][self.f[\"hourly_index\"] == j]\n","        ).float()\n","        item[\"survival\"] = torch.tensor(\n","            self.f[\"survival\"][self.f[\"hourly_index\"] == j]\n","        )\n","        return item\n","\n","    def pad_bincount(self, records):\n","        # get counts of each cod\n","        records = np.bincount(records)\n","        # pad each vector to length T, all possible codes\n","        padded = np.zeros(self.n_codes)\n","        padded[: len(records)] = records\n","        return torch.from_numpy(padded).float()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def padded_collate(batch, pad_index, causal=False):\n","    res = {}\n","    treatment = torch.tensor(np.array([d[\"treatment\"] for d in batch]))\n","    demog = torch.tensor(np.array([d[\"demog\"] for d in batch])).float()\n","    if causal:\n","        res[\"treatment\"] = torch.tensor(np.array([d[\"treatment\"] for d in batch]))\n","        res[\"static\"] = torch.tensor(np.array([d[\"demog\"] for d in batch])).float()\n","    else:\n","        res[\"static\"] = torch.cat([demog, treatment.unsqueeze(1)], 1)\n","    res[\"codes\"] = torch.stack([d[\"codes\"] for d in batch])\n","    res[\"vitals\"] = pad_sequence(\n","        [d[\"vitals\"] for d in batch], batch_first=True, padding_value=pad_index\n","    )\n","    res[\"survival\"] = pad_sequence(\n","        [d[\"survival\"] for d in batch], batch_first=True, padding_value=pad_index\n","    )\n","    return res"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Dynamic Survival Transformers Model Class"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### First some housekeeping"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Import the os module\n","import os\n","\n","# Get the current working directory\n","cwd = os.getcwd()\n","\n","# Print the current working directory\n","print(\"Current working directory: {0}\".format(cwd))\n","\n","# Install PyTorch Lightning\n","%pip install pytorch-lightning\n","\n","# Install modules and libraries\n","import pytorch_lightning as pl\n","import torch\n","import math\n","from torch.nn import Linear\n","from src.metric import MeanAbsoluteError, ConcordanceIndex"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### DST Model Class"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":169,"status":"ok","timestamp":1681671979884,"user":{"displayName":"Michael Conlin","userId":"06177671838518648738"},"user_tz":420},"id":"7YLYUTZ2fLn2"},"outputs":[],"source":["class DST(pl.LightningModule):\n","    def __init__(\n","        self,\n","        n_codes,\n","        n_vitals,\n","        n_demog,\n","        d_model,\n","        n_blocks,\n","        n_heads,\n","        dropout,\n","        pad,\n","        dynamic,\n","        causal,\n","        lr=0.001,\n","        alpha=0.01,\n","    ):\n","        super().__init__()\n","        self.save_hyperparameters()\n","        self.embed_codes = Linear(n_codes, d_model)\n","        d1 = 0 if causal else 1\n","        self.embed_static = Linear(d_model + n_demog + d1, d_model)\n","        self.embed_vitals = Linear(n_vitals, d_model)\n","        self.pos_encode = PositionalEncoding(d_model)\n","        self.pad = pad\n","        encoder_layer = torch.nn.TransformerEncoderLayer(\n","            d_model = d_model,\n","            nhead=n_heads,\n","            dropout=dropout,\n","            batch_first=True,\n","            dim_feedforward=d_model*4\n","        )\n","        norm = torch.nn.LayerNorm(d_model)\n","        self.transformer = torch.nn.TransformerEncoder(encoder_layer, n_blocks, norm)\n","        d2 = 1 if causal else 0\n","        self.to_hazard_c = torch.nn.Sequential(\n","            Linear(d_model + d2, d_model//2),\n","            torch.nn.ReLU(),\n","            Linear(d_model//2, 1),\n","            torch.nn.Sigmoid(),\n","        )\n","        self.train_mae = MeanAbsoluteError(pad=pad)\n","        self.val_mae = MeanAbsoluteError(pad=pad)\n","        self.val_ci = ConcordanceIndex(pad=pad)\n","        self.test_mae = MeanAbsoluteError(pad=pad)\n","        self.test_ci = ConcordanceIndex(pad=pad)\n","\n","        # how much to weigh MAE loss\n","        self.alpha = alpha\n","        self.dynamic = dynamic\n","        self.causal = causal\n","\n","\n","    def forward(self, batch):\n","        # static features\n","        x = self.embed_codes(batch[\"codes\"]).unsqueeze(1)\n","        x = self.embed_static(\n","            torch.cat([x, batch[\"static\"].unsqueeze(1)], 2)\n","        )\n","        s = batch[\"vitals\"].shape[1]\n","        # time-varying features\n","        if self.dynamic:\n","            pad_mask = (batch[\"vitals\"][:, :, 0] == self.pad)\n","            x = x + self.embed_vitals(batch[\"vitals\"])\n","            # autoregressive mask\n","            mask = (1 - torch.tril(torch.ones(s, s))).bool().cuda()\n","\n","        else:\n","            mask = None\n","            x = x.repeat(1, s, 1)\n","            pad_mask = (batch[\"vitals\"][:, :, 0] == self.pad)\n","        x = self.pos_encode(x)\n","        x = self.transformer(x, mask, pad_mask)\n","        if self.causal:\n","            t = torch.reshape(batch[\"treatment\"], (-1, 1, 1))\n","            t = t.repeat(1, s, 1)\n","            # concatenate treatment as a new feature\n","            x = torch.cat((x, t), 2).float()\n","        # complement of hazard\n","        q_hat = self.to_hazard_c(x).squeeze(2)\n","        s_hat = q_hat.cumprod(1).clamp(min=1e-8)\n","        return s_hat\n","\n","    def training_step(self, batch, batch_idx):\n","        s_hat =  self(batch)\n","        loss = self.combined_loss(s_hat, batch[\"survival\"])\n","        self.log(\"train_loss\", loss)\n","        self.train_mae(s_hat, batch[\"survival\"])\n","        self.log(\"train_mae\", self.train_mae, on_step=True, on_epoch=False)\n","        return loss\n","\n","\n","\n","    def validation_step(self, batch, batch_idx):\n","        s_hat =  self(batch)\n","        loss = self.combined_loss(s_hat, batch[\"survival\"])\n","        self.val_mae.update(s_hat, batch[\"survival\"])\n","        self.val_ci.update(s_hat, batch[\"survival\"])\n","        self.log(\"val_loss\", loss)\n","        self.log(\"val_mae\", self.val_mae, on_step=True, on_epoch=True)\n","        self.log(\"val_ci\", self.val_ci, on_step=True, on_epoch=True)\n","        return loss\n","\n","\n","    def test_step(self, batch, batch_idx):\n","        s_hat =  self(batch)\n","        loss = self.combined_loss(s_hat, batch[\"survival\"])\n","        self.test_mae.update(s_hat, batch[\"survival\"])\n","        self.test_ci.update(s_hat, batch[\"survival\"])\n","        self.log(\"test_mae\", self.test_mae, on_step=True, on_epoch=True)\n","        self.log(\"test_ci\", self.test_ci, on_step=True, on_epoch=True)\n","        return loss\n","\n","    def predict_step(self, batch, batch_idx):\n","        # returns estimated survival times\n","        s_hat = self(batch)\n","        mask = (batch[\"survival\"] != self.pad)\n","        return (s_hat * mask).sum(1)\n","\n","\n","    def configure_optimizers(self):\n","        return torch.optim.AdamW(self.parameters(), lr=self.hparams.lr)\n","\n","    def ordinal_survival_loss(self, s_hat, y):\n","        # modified cross entropy loss\n","        nlog_survival = -torch.log(s_hat)\n","        nlog_failure = -torch.log(1 - s_hat)\n","        loss = 0\n","        loss += nlog_survival * torch.where(y==self.pad, 0, y)\n","        loss += nlog_failure * torch.where(y==self.pad, 0, (1-y))\n","        return loss.sum() / (y != self.pad).sum()\n","    \n","    def mae_loss(self, s_hat, y):\n","        observed = (y == 0).any(1).int()\n","        t_hat = torch.where(y == self.pad, 0, s_hat).sum(1)\n","        t = torch.where(y == self.pad, 0, y).sum(1)\n","        zeros = torch.zeros(t.shape).cuda()\n","        observed_error = torch.abs(t_hat - t) * observed\n","        censored_error = torch.maximum(zeros, t - t_hat) * (1 - observed)\n","        return (observed_error.sum() + censored_error.sum()) / t.numel()\n","\n","    def combined_loss(self, s_hat, y):\n","        a = self.alpha\n","        ordinal_loss = self.ordinal_survival_loss(s_hat, y)\n","        mae_loss = self.mae_loss(s_hat, y)\n","        return (1 - a) * ordinal_loss + a * mae_loss\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### Define the positional encoding class"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":138,"status":"ok","timestamp":1681671986002,"user":{"displayName":"Michael Conlin","userId":"06177671838518648738"},"user_tz":420},"id":"RTV0nv6rfcVq"},"outputs":[],"source":["class PositionalEncoding(torch.nn.Module):\n","\n","    def __init__(self, d_model, dropout=0.1, max_len=5000):\n","        super(PositionalEncoding, self).__init__()\n","        self.dropout = torch.nn.Dropout(p=dropout)\n","\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        x = x + self.pe[:x.size(1), :].unsqueeze(0)\n","        return self.dropout(x)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### Set up Weights and Biases"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7682,"status":"ok","timestamp":1681678913339,"user":{"displayName":"Michael Conlin","userId":"06177671838518648738"},"user_tz":420},"id":"OOev89fK1MxK","outputId":"657c8dbf-9346-40ad-88d8-aa6f554f9164"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.2/199.2 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["%pip install wandb -qU"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":70},"executionInfo":{"elapsed":5018,"status":"ok","timestamp":1681678934974,"user":{"displayName":"Michael Conlin","userId":"06177671838518648738"},"user_tz":420},"id":"LPFaB18f3_18","outputId":"469673a6-6c6c-4703-d183-c8b1933a0ae2"},"outputs":[{"data":{"application/javascript":"\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    ","text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"text/plain":["True"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["# Log in to your W&B account\n","import wandb\n","wandb.login()"]},{"cell_type":"markdown","metadata":{"id":"2aa2Jt-A734N"},"source":["# New Section"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# import torch\n","from torch.utils.data import DataLoader, random_split\n","# import pytorch_lightning as pl\n","from pytorch_lightning.callbacks import ModelCheckpoint\n","\n","# set the default parameters\n","\n","# preprocess\n","preprocess_seed = 30\n","\n","# train\n","train_frac = .7\n","val_frac = .15\n","accelerator = \"gpu\"\n","devices = 1\n","max_epochs = 5\n","batch_size = 32\n","train_seed = 0\n","\n","causal = False\n","\n","# model\n","_target_ = \"src.model.DST\"\n","d_model = 32\n","n_blocks = 3\n","n_heads = 8\n","dropout = .1\n","pad = -100\n","dynamic = True\n","lr = 0.001\n","alpha = 0.01\n","\n","# This call to the Mimi3Dataset class relies on having already run the preprocessing pipeline. If you have not run the preprocessing notebook, this will not work.\n","\n","dataset = Mimic3Dataset(work_dir=cwd, seed=preprocess_seed)\n","\n","train_size = int(train_frac * len(dataset))\n","if train_frac + val_frac == 1.0:\n","    val_size = len(dataset) - train_size\n","    test_size = 0\n","else:\n","    val_size = int(val_frac * len(dataset))\n","    test_size = len(dataset) - train_size - val_size\n","train_set, val_set, test_set = random_split(\n","    dataset,\n","    (train_size, val_size, test_size),\n","    torch.Generator().manual_seed(train_seed)\n",")\n","\n","###### new code\n","\n","def collate(x):\n","    return padded_collate(x, pad_index=pad, causal=causal)\n","train_loader = DataLoader(\n","    train_set,\n","    collate_fn=collate,\n","    batch_size=batch_size,\n","    shuffle=True\n","    )\n","val_loader = DataLoader(\n","    val_set, collate_fn=collate, batch_size=batch_size\n",")\n","if test_size:\n","    test_loader = DataLoader(\n","        test_set, collate_fn=collate, batch_size=batch_size\n","    )\n","\n","model = DST(n_codes=dataset.n_codes, n_vitals=dataset.n_vitals, n_demog=dataset.n_demog, d_model=d_model, n_blocks=n_blocks, n_heads=n_heads, dropout=dropout, pad=pad, dynamic=dynamic, lr=lr, alpha=alpha, causal=causal)\n","\n","callbacks = [ModelCheckpoint(monitor=\"val_mae_epoch\", mode=\"min\")]\n","trainer = pl.Trainer(\n","    accelerator=accelerator,\n","    devices=devices,\n","    max_epochs=max_epochs,\n","    callbacks=callbacks,\n",")\n","trainer.fit(model, train_loader, val_loader)\n","if test_size:\n","    trainer.test(dataloaders=test_loader)"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyM4f4HqebxS086cY4L2R9hZ","provenance":[]},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
